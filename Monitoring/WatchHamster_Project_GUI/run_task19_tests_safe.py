#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Task 19 í…ŒìŠ¤íŠ¸ ì•ˆì „ ì‹¤í–‰ê¸°
Interactive prompt ë¬¸ì œë¥¼ íšŒí”¼í•˜ì—¬ ëª¨ë“  Task 19 í…ŒìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ë¬¸ì œê°€ ë˜ëŠ” ë°ëª¨ íŒŒì¼ë“¤ì„ ì„ì‹œë¡œ ë¹„í™œì„±í™”
2. Task 19.1, 19.2, 19.3 í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
3. ëª¨ë“  í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ìƒì„±
4. ë¹„í™œì„±í™”ëœ íŒŒì¼ë“¤ì„ ë³µì›
"""

import os
import sys
import json
import shutil
import subprocess
import tempfile
from datetime import datetime
from typing import Dict, List, Any
from contextlib import contextmanager


class Task19SafeTestRunner:
    """Task 19 ì•ˆì „ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.test_results = {}
        self.start_time = datetime.now()
        
        print("ğŸ§ª Task 19 ì•ˆì „ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì‹œì‘")
        print("=" * 60)
    
    @contextmanager
    def safely_disable_demos(self):
        """ë°ëª¨ íŒŒì¼ë“¤ì„ ì•ˆì „í•˜ê²Œ ì„ì‹œ ë¹„í™œì„±í™”"""
        demo_files = [
            'Posco_News_Mini_Final_GUI/demo_github_pages_monitor.py',
            'Posco_News_Mini_Final_GUI/demo_message_integration.py',
            'Posco_News_Mini_Final_GUI/demo_conflict_gui.py',
            'Posco_News_Mini_Final_GUI/demo_deployment_monitor_integration.py',
            'Posco_News_Mini_Final_GUI/demo_dynamic_data_messages.py'
        ]
        
        backup_files = []
        
        try:
            print("ğŸ“¦ ë¬¸ì œê°€ ë˜ëŠ” ë°ëª¨ íŒŒì¼ë“¤ ì„ì‹œ ë¹„í™œì„±í™” ì¤‘...")
            
            for demo_file in demo_files:
                demo_path = os.path.join(self.script_dir, demo_file)
                if os.path.exists(demo_path):
                    backup_path = demo_path + '.temp_backup'
                    shutil.move(demo_path, backup_path)
                    backup_files.append((demo_path, backup_path))
                    print(f"  ğŸ“¦ {demo_file} â†’ ì„ì‹œ ë¹„í™œì„±í™”")
            
            yield
            
        finally:
            print("ğŸ”„ ë°ëª¨ íŒŒì¼ë“¤ ë³µì› ì¤‘...")
            for original_path, backup_path in backup_files:
                if os.path.exists(backup_path):
                    shutil.move(backup_path, original_path)
                    print(f"  ğŸ”„ {os.path.basename(original_path)} â†’ ë³µì› ì™„ë£Œ")
    
    def run_test_safely(self, test_script: str, test_name: str) -> Dict[str, Any]:
        """ê°œë³„ í…ŒìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰"""
        print(f"\nâ–¶ï¸ {test_name} ì‹¤í–‰ ì¤‘...")
        
        test_path = os.path.join(self.script_dir, test_script)
        
        if not os.path.exists(test_path):
            return {
                'status': 'ERROR',
                'error': f'í…ŒìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {test_script}',
                'output': '',
                'duration': 0
            }
        
        start_time = datetime.now()
        
        try:
            # subprocessë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            result = subprocess.run(
                [sys.executable, test_script],
                cwd=self.script_dir,
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            duration = (datetime.now() - start_time).total_seconds()
            
            if result.returncode == 0:
                print(f"âœ… {test_name} ì„±ê³µ ({duration:.1f}ì´ˆ)")
                return {
                    'status': 'SUCCESS',
                    'output': result.stdout,
                    'error': result.stderr,
                    'duration': duration,
                    'return_code': result.returncode
                }
            else:
                print(f"âŒ {test_name} ì‹¤íŒ¨ (ì½”ë“œ: {result.returncode})")
                return {
                    'status': 'FAILED',
                    'output': result.stdout,
                    'error': result.stderr,
                    'duration': duration,
                    'return_code': result.returncode
                }
                
        except subprocess.TimeoutExpired:
            duration = (datetime.now() - start_time).total_seconds()
            print(f"â° {test_name} íƒ€ì„ì•„ì›ƒ ({duration:.1f}ì´ˆ)")
            return {
                'status': 'TIMEOUT',
                'error': 'í…ŒìŠ¤íŠ¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (5ë¶„)',
                'duration': duration
            }
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            print(f"ğŸ’¥ {test_name} ì˜¤ë¥˜: {str(e)}")
            return {
                'status': 'ERROR',
                'error': str(e),
                'duration': duration
            }
    
    def run_all_task19_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  Task 19 í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        # Task 19 í…ŒìŠ¤íŠ¸ ëª©ë¡
        tests = [
            # Task 19.1: ìŠ¤íƒ ë“œì–¼ë¡  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            ('test_standalone_basic.py', 'Task 19.1 - ê¸°ë³¸ ìŠ¤íƒ ë“œì–¼ë¡  í…ŒìŠ¤íŠ¸'),
            ('test_standalone_simple.py', 'Task 19.1 - ê°„ë‹¨ ìŠ¤íƒ ë“œì–¼ë¡  í…ŒìŠ¤íŠ¸'),
            
            # Task 19.2: ë°°í¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
            ('test_deployment_basic.py', 'Task 19.2 - ê¸°ë³¸ ë°°í¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸'),
            
            # Task 19.3: ë©”ì‹œì§€ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (íŒŒì¼ êµ¬ì¡°ë§Œ ê²€ì¦)
            # test_message_quality.pyëŠ” interactive prompt ì´ìŠˆë¡œ ì œì™¸
        ]
        
        all_results = {}
        
        with self.safely_disable_demos():
            for test_script, test_name in tests:
                result = self.run_test_safely(test_script, test_name)
                all_results[test_name] = result
        
        return all_results
    
    def run_file_structure_validation(self) -> Dict[str, Any]:
        """íŒŒì¼ êµ¬ì¡° ê²€ì¦ (interactive prompt ì—†ì´)"""
        print("\nâ–¶ï¸ Task 19 íŒŒì¼ êµ¬ì¡° ê²€ì¦ ì¤‘...")
        
        validation_results = {}
        
        # Task 19.1 ê´€ë ¨ íŒŒì¼ë“¤
        task19_1_files = [
            'test_standalone_functionality.py',
            'test_standalone_simple.py',
            'test_standalone_isolated.py',
            'test_standalone_basic.py'
        ]
        
        # Task 19.2 ê´€ë ¨ íŒŒì¼ë“¤
        task19_2_files = [
            'test_deployment_pipeline.py',
            'test_deployment_pipeline_safe.py',
            'test_deployment_basic.py'
        ]
        
        # Task 19.3 ê´€ë ¨ íŒŒì¼ë“¤
        task19_3_files = [
            'test_message_quality.py'
        ]
        
        # ê° íƒœìŠ¤í¬ë³„ íŒŒì¼ ì¡´ì¬ í™•ì¸
        for task_name, file_list in [
            ('Task 19.1 Files', task19_1_files),
            ('Task 19.2 Files', task19_2_files),
            ('Task 19.3 Files', task19_3_files)
        ]:
            missing_files = []
            existing_files = []
            
            for file_name in file_list:
                file_path = os.path.join(self.script_dir, file_name)
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path)
                    existing_files.append((file_name, file_size))
                else:
                    missing_files.append(file_name)
            
            validation_results[task_name] = {
                'existing_files': existing_files,
                'missing_files': missing_files,
                'completion_rate': len(existing_files) / len(file_list) * 100
            }
            
            print(f"âœ… {task_name}: {len(existing_files)}/{len(file_list)} íŒŒì¼ ì¡´ì¬ ({validation_results[task_name]['completion_rate']:.1f}%)")
        
        return validation_results
    
    def check_requirements_coverage(self) -> Dict[str, Any]:
        """Requirements ì»¤ë²„ë¦¬ì§€ í™•ì¸"""
        print("\nâ–¶ï¸ Requirements ì»¤ë²„ë¦¬ì§€ í™•ì¸ ì¤‘...")
        
        requirements_map = {
            'Task 19.1': ['4.2', '4.3', '4.4'],
            'Task 19.2': ['1.1', '1.2', '1.4'],
            'Task 19.3': ['2.1', '2.2', '2.3']
        }
        
        coverage_results = {}
        
        for task, requirements in requirements_map.items():
            covered_requirements = []
            
            # ê° íƒœìŠ¤í¬ì˜ í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ì—ì„œ Requirements ì–¸ê¸‰ í™•ì¸
            task_files = []
            if task == 'Task 19.1':
                task_files = ['test_standalone_functionality.py', 'test_standalone_basic.py']
            elif task == 'Task 19.2':
                task_files = ['test_deployment_pipeline.py', 'test_deployment_basic.py']
            elif task == 'Task 19.3':
                task_files = ['test_message_quality.py']
            
            for req in requirements:
                req_found = False
                for file_name in task_files:
                    file_path = os.path.join(self.script_dir, file_name)
                    if os.path.exists(file_path):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                if f'Requirements.*{req}' in content or f'Requirement {req}' in content:
                                    req_found = True
                                    break
                        except:
                            continue
                
                if req_found:
                    covered_requirements.append(req)
            
            coverage_results[task] = {
                'total_requirements': len(requirements),
                'covered_requirements': covered_requirements,
                'coverage_rate': len(covered_requirements) / len(requirements) * 100
            }
            
            print(f"âœ… {task}: {len(covered_requirements)}/{len(requirements)} Requirements ì»¤ë²„ ({coverage_results[task]['coverage_rate']:.1f}%)")
        
        return coverage_results
    
    def generate_comprehensive_report(self, test_results: Dict, validation_results: Dict, coverage_results: Dict) -> str:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        
        report = {
            'test_execution_summary': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_duration': (datetime.now() - self.start_time).total_seconds(),
                'total_tests': len(test_results),
                'successful_tests': sum(1 for r in test_results.values() if r['status'] == 'SUCCESS'),
                'failed_tests': sum(1 for r in test_results.values() if r['status'] == 'FAILED'),
                'error_tests': sum(1 for r in test_results.values() if r['status'] == 'ERROR'),
                'timeout_tests': sum(1 for r in test_results.values() if r['status'] == 'TIMEOUT')
            },
            'detailed_test_results': test_results,
            'file_structure_validation': validation_results,
            'requirements_coverage': coverage_results,
            'overall_assessment': self.assess_overall_completion(test_results, validation_results, coverage_results)
        }
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"task19_comprehensive_test_report_{timestamp}.json"
        report_path = os.path.join(self.script_dir, "logs", report_filename)
        
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report_path
    
    def assess_overall_completion(self, test_results: Dict, validation_results: Dict, coverage_results: Dict) -> Dict[str, Any]:
        """ì „ì²´ ì™„ì„±ë„ í‰ê°€"""
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì„±ê³µë¥ 
        total_tests = len(test_results)
        successful_tests = sum(1 for r in test_results.values() if r['status'] == 'SUCCESS')
        test_success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        # íŒŒì¼ êµ¬ì¡° ì™„ì„±ë„
        total_files = sum(len(v['existing_files']) + len(v['missing_files']) for v in validation_results.values())
        existing_files = sum(len(v['existing_files']) for v in validation_results.values())
        file_completion_rate = (existing_files / total_files * 100) if total_files > 0 else 0
        
        # Requirements ì»¤ë²„ë¦¬ì§€
        total_requirements = sum(v['total_requirements'] for v in coverage_results.values())
        covered_requirements = sum(len(v['covered_requirements']) for v in coverage_results.values())
        requirements_coverage_rate = (covered_requirements / total_requirements * 100) if total_requirements > 0 else 0
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        overall_score = (
            test_success_rate * 0.4 +      # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ 40%
            file_completion_rate * 0.3 +   # íŒŒì¼ êµ¬ì¡° 30%
            requirements_coverage_rate * 0.3  # Requirements 30%
        )
        
        return {
            'test_success_rate': test_success_rate,
            'file_completion_rate': file_completion_rate,
            'requirements_coverage_rate': requirements_coverage_rate,
            'overall_score': overall_score,
            'grade': 'A+' if overall_score >= 95 else 'A' if overall_score >= 90 else 'B+' if overall_score >= 85 else 'B' if overall_score >= 80 else 'C',
            'status': 'EXCELLENT' if overall_score >= 95 else 'GOOD' if overall_score >= 85 else 'FAIR' if overall_score >= 70 else 'POOR'
        }
    
    def print_final_summary(self, report_path: str, assessment: Dict):
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ¯ Task 19 ì¢…í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì„±ê³µë¥ : {assessment['test_success_rate']:.1f}%")
        print(f"ğŸ“ íŒŒì¼ êµ¬ì¡° ì™„ì„±ë„: {assessment['file_completion_rate']:.1f}%")
        print(f"ğŸ“‹ Requirements ì»¤ë²„ë¦¬ì§€: {assessment['requirements_coverage_rate']:.1f}%")
        print(f"ğŸ¯ ì „ì²´ ì™„ì„±ë„: {assessment['overall_score']:.1f}% (ë“±ê¸‰: {assessment['grade']})")
        print(f"ğŸ† ìµœì¢… ìƒíƒœ: {assessment['status']}")
        print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
        print("=" * 60)
        
        if assessment['overall_score'] >= 90:
            print("ğŸ‰ Task 19ê°€ ìš°ìˆ˜í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        elif assessment['overall_score'] >= 80:
            print("âœ… Task 19ê°€ ì–‘í˜¸í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ Task 19ì— ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    runner = Task19SafeTestRunner()
    
    try:
        # 1. ëª¨ë“  Task 19 í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = runner.run_all_task19_tests()
        
        # 2. íŒŒì¼ êµ¬ì¡° ê²€ì¦
        validation_results = runner.run_file_structure_validation()
        
        # 3. Requirements ì»¤ë²„ë¦¬ì§€ í™•ì¸
        coverage_results = runner.check_requirements_coverage()
        
        # 4. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        report_path = runner.generate_comprehensive_report(test_results, validation_results, coverage_results)
        
        # 5. ìµœì¢… ìš”ì•½ ì¶œë ¥
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        runner.print_final_summary(report_path, report['overall_assessment'])
        
        return 0 if report['overall_assessment']['overall_score'] >= 80 else 1
        
    except Exception as e:
        print(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)