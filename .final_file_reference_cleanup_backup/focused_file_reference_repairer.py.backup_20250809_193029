#!/usr/bin/env python3
"""
POSCO 시스템 파일 참조 무결성 집중 복구 도구

Task 4: 파일 참조 무결성 완전 복구
- 핵심 Python 파일의 깨진 import 수정
- 실행 스크립트의 파일 참조 수정
- 중요 설정 파일의 경로 참조 수정
"""

import os
import re
import json
import shutil
import ast
from pathlib import Path
from typing import Dict, List, Set, Optional
from dataclasses import dataclass
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class CriticalFileReference:
    """중요한 파일 참조 문제"""
    source_file: str
    referenced_path: str
    line_number: int
    issue_type: str
    suggested_fix: Optional[str] = None

class FocusedFileReferenceRepairer:
    """집중적인 파일 참조 복구 클래스"""
    
    def __init__(self, root_path: str = "."):
        self.root_path = Path(root_path).resolve()
        self.critical_issues: List[CriticalFileReference] = []
        self.existing_files: Dict[str, str] = {}
        self.backup_dir = Path(".focused_file_reference_backup")
        
        # 핵심 파일들만 대상으로 함
        self.critical_files = [
            "file_renaming_system.py",
            "filename_standardizer.py", 
            "naming_convention_manager.py",
            "python_naming_standardizer.py",
            "shell_batch_script_standardizer.py",
            "documentation_standardizer.py",
            "config_data_standardizer.py",
            "system_output_message_standardizer.py",
            "folder_structure_reorganizer.py",
            "naming_standardization_verification_system.py",
            "POSCO_News_250808.py",
            "final_integration_test_system.py",
            "system_functionality_verification.py"
        ]
        
        self._build_file_index()

    def _build_file_index(self):
        """핵심 파일들의 인덱스 구축"""
        logger.info("핵심 파일 인덱스 구축 중...")
        
        for file_path in self.root_path.rglob("*.py"):
            if (file_path.is_file() and 
                not file_path.name.startswith('.') and
                not any(exclude in str(file_path) for exclude in ['.git', '__pycache__', 'backup'])):
                
                filename = file_path.name
                stem = file_path.stem
                relative_path = str(file_path.relative_to(self.root_path))
                
                self.existing_files[filename] = relative_path
                self.existing_files[stem] = relative_path

    def scan_critical_references(self) -> List[CriticalFileReference]:
        """핵심 파일들의 참조 스캔"""
        logger.info("핵심 파일 참조 스캔 시작...")
        
        for critical_file in self.critical_files:
            file_paths = list(self.root_path.rglob(critical_file))
            for file_path in file_paths:
                if file_path.is_file():
                    self._scan_critical_file(file_path)
        
        logger.info(f"총 {len(self.critical_issues)}개의 핵심 참조 문제 발견")
        return self.critical_issues

    def _scan_critical_file(self, file_path: Path):
        """핵심 파일 스캔"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            # import 문 검사
            for line_num, line in enumerate(lines, 1):
                if 'import ' in line and not line.strip().startswith('#'):
                    self._check_import_line(line, file_path, line_num)
                
                # 파일 경로 참조 검사
                self._check_file_path_line(line, file_path, line_num)
                
        except Exception as e:
            logger.error(f"핵심 파일 스캔 오류 {file_path}: {e}")

    def _check_import_line(self, line: str, source_file: Path, line_num: int):
        """import 라인 검사"""
        # 명확한 깨진 import만 찾기
        broken_imports = [
            'test_config.json',
            'posco_news_250808_monitor.log',
            'verify_folder_reorganization.py',
            'glob',
            'datetime'
        ]
        
        for broken_import in broken_imports:
            if broken_import in line:
                suggested_fix = self._get_import_fix(broken_import)
                if suggested_fix:
                    issue = CriticalFileReference(
                        source_file=str(source_file.relative_to(self.root_path)),
                        referenced_path=broken_import,
                        line_number=line_num,
                        issue_type='broken_import',
                        suggested_fix=suggested_fix
                    )
                    self.critical_issues.append(issue)

    def _check_file_path_line(self, line: str, source_file: Path, line_num: int):
        """파일 경로 라인 검사"""
        # 명확한 파일 경로 패턴
        patterns = [
            r'[\'"]([^\'"\s]+\.py)[\'"]',
            r'[\'"]([^\'"\s]+\.json)[\'"]',
            r'[\'"]([^\'"\s]+\.log)[\'"]'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, line)
            for match in matches:
                file_ref = match.group(1)
                if not self._file_exists_simple(file_ref) and self._is_fixable(file_ref):
                    suggested_fix = self._get_file_fix(file_ref)
                    if suggested_fix:
                        issue = CriticalFileReference(
                            source_file=str(source_file.relative_to(self.root_path)),
                            referenced_path=file_ref,
                            line_number=line_num,
                            issue_type='missing_file',
                            suggested_fix=suggested_fix
                        )
                        self.critical_issues.append(issue)

    def _get_import_fix(self, broken_import: str) -> Optional[str]:
        """import 수정 방안"""
        fixes = {
            'test_config.json': '',  # 제거
            'posco_news_250808_monitor.log': '',  # 제거
            'verify_folder_reorganization.py': '',  # 제거
            'glob': 'import glob',  # 표준 라이브러리
            'datetime': 'from datetime import datetime'  # 표준 라이브러리
        }
        return fixes.get(broken_import)

    def _get_file_fix(self, file_ref: str) -> Optional[str]:
        """파일 경로 수정 방안"""
        filename = Path(file_ref).name
        
        # 직접 매칭
        if filename in self.existing_files:
            return self.existing_files[filename]
        
        # 유사한 파일 찾기
        for existing_file, path in self.existing_files.items():
            if (filename.lower() in existing_file.lower() or
                existing_file.lower() in filename.lower()):
                return path
        
        return None

    def _file_exists_simple(self, file_ref: str) -> bool:
        """간단한 파일 존재 확인"""
        # 루트 기준 경로
        root_path = self.root_path / file_ref
        if root_path.exists():
            return True
        
        # 파일명만으로 확인
        filename = Path(file_ref).name
        return filename in self.existing_files

    def _is_fixable(self, file_ref: str) -> bool:
        """수정 가능한 참조인지 확인"""
        # 특정 패턴은 제외
        exclude_patterns = [
            'http://', 'https://', 'file://',
            '__pycache__', '.git/', 'backup',
            '*.', '{', '}', '$'
        ]
        
        for pattern in exclude_patterns:
            if pattern in file_ref:
                return False
        
        return True

    def repair_critical_references(self) -> Dict:
        """핵심 참조 수리"""
        logger.info("핵심 파일 참조 수리 시작...")
        
        repair_results = []
        
        # 파일별로 그룹화
        files_to_repair = {}
        for issue in self.critical_issues:
            if issue.source_file not in files_to_repair:
                files_to_repair[issue.source_file] = []
            files_to_repair[issue.source_file].append(issue)
        
        # 각 파일 수리
        for file_path, issues in files_to_repair.items():
            result = self._repair_file(file_path, issues)
            repair_results.append(result)
        
        logger.info(f"총 {len(repair_results)}개 파일 수리 완료")
        return {
            "repaired_files": len(repair_results),
            "successful_repairs": len([r for r in repair_results if r['success']]),
            "total_changes": sum(len(r['changes']) for r in repair_results if r['success']),
            "results": repair_results
        }

    def _repair_file(self, file_path: str, issues: List[CriticalFileReference]) -> Dict:
        """개별 파일 수리"""
        full_path = self.root_path / file_path
        changes = []
        
        try:
            # 백업 생성
            self._create_backup(full_path)
            
            # 파일 읽기
            with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            # 라인별 수정
            for issue in sorted(issues, key=lambda x: x.line_number, reverse=True):
                if issue.line_number <= len(lines) and issue.suggested_fix is not None:
                    old_line = lines[issue.line_number - 1]
                    
                    if issue.suggested_fix == '':  # 제거
                        # 라인을 주석 처리
                        if not old_line.strip().startswith('#'):
                            lines[issue.line_number - 1] = f"# REMOVED: {old_line}"
                            changes.append(f"Line {issue.line_number}: 제거 - {issue.referenced_path}")
                    else:  # 교체
                        new_line = old_line.replace(issue.referenced_path, issue.suggested_fix)
                        if new_line != old_line:
                            lines[issue.line_number - 1] = new_line
                            changes.append(f"Line {issue.line_number}: {issue.referenced_path} → {issue.suggested_fix}")
            
            # 파일 쓰기
            if changes:
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.writelines(lines)
            
            return {
                "file_path": file_path,
                "success": True,
                "changes": changes
            }
            
        except Exception as e:
            return {
                "file_path": file_path,
                "success": False,
                "error": str(e),
                "changes": []
            }

    def _create_backup(self, file_path: Path):
        """백업 생성"""
        try:
            if not self.backup_dir.exists():
                self.backup_dir.mkdir(parents=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.backup_dir / f"{file_path.name}.backup_{timestamp}"
            shutil.copy2(file_path, backup_path)
        except Exception as e:
            logger.error(f"백업 생성 실패 {file_path}: {e}")

    def generate_focused_report(self) -> Dict:
        """집중 보고서 생성"""
        report = {
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "summary": {
                "critical_files_scanned": len(self.critical_files),
                "critical_issues_found": len(self.critical_issues),
                "broken_imports": len([i for i in self.critical_issues if i.issue_type == 'broken_import']),
                "missing_files": len([i for i in self.critical_issues if i.issue_type == 'missing_file'])
            },
            "critical_issues": [
                {
                    "source_file": issue.source_file,
                    "referenced_path": issue.referenced_path,
                    "line_number": issue.line_number,
                    "issue_type": issue.issue_type,
                    "suggested_fix": issue.suggested_fix
                }
                for issue in self.critical_issues
            ]
        }
        
        return report

    def save_report(self, report: Dict, filename: str = "focused_file_reference_repair_report.json"):
        """보고서 저장"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        logger.info(f"보고서 저장: {filename}")

def main():
    """메인 실행 함수"""
    print("🎯 POSCO 시스템 핵심 파일 참조 집중 복구 도구")
    print("=" * 60)
    
    repairer = FocusedFileReferenceRepairer()
    
    # 1. 핵심 파일 참조 스캔
    print("\n1️⃣ 핵심 파일 참조 스캔 중...")
    critical_issues = repairer.scan_critical_references()
    
    if critical_issues:
        print(f"   ⚠️  {len(critical_issues)}개의 핵심 참조 문제 발견")
        
        # 문제 유형별 분석
        broken_imports = [i for i in critical_issues if i.issue_type == 'broken_import']
        missing_files = [i for i in critical_issues if i.issue_type == 'missing_file']
        
        print(f"   • 깨진 import: {len(broken_imports)}개")
        print(f"   • 누락된 파일: {len(missing_files)}개")
        
        # 2. 핵심 참조 수리
        print(f"\n2️⃣ {len(critical_issues)}개의 핵심 참조 수리 중...")
        repair_result = repairer.repair_critical_references()
        print(f"   ✅ {repair_result['successful_repairs']}/{repair_result['repaired_files']}개 파일 수리 완료")
        print(f"   📝 총 {repair_result['total_changes']}개 변경 사항 적용")
        
        # 3. 집중 보고서 생성
        print("\n3️⃣ 집중 수리 보고서 생성 중...")
        report = repairer.generate_focused_report()
        report['repair_results'] = repair_result
        repairer.save_report(report)
        print("   ✅ 보고서 생성 완료")
        
        # 결과 요약
        print(f"\n📊 핵심 수리 결과:")
        print(f"   • 스캔된 핵심 파일: {len(repairer.critical_files)}개")
        print(f"   • 발견된 문제: {len(critical_issues)}개")
        print(f"   • 수리된 파일: {repair_result['successful_repairs']}개")
        print(f"   • 적용된 변경: {repair_result['total_changes']}개")
        print(f"   • 백업 디렉토리: {repairer.backup_dir}")
        
        # 주요 수리 내용
        if repair_result['successful_repairs'] > 0:
            print(f"\n🎉 주요 수리 성과:")
            for result in repair_result['results']:
                if result['success'] and result['changes']:
                    print(f"   • {result['file_path']}: {len(result['changes'])}개 수정")
    else:
        print("   ✅ 핵심 파일 참조 문제가 발견되지 않았습니다!")
    
    return len(critical_issues) == 0 or repair_result.get('successful_repairs', 0) > 0

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)