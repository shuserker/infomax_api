#!/usr/bin/env python3
"""
POSCO ì‹œìŠ¤í…œ íŒŒì¼ ë¶„ë¥˜ ë° ë¶„ì„ ì‹œìŠ¤í…œ
File Classification and Analysis System

2000+ íŒŒì¼ë“¤ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ì •ë¦¬ë¥¼ ìœ„í•œ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ë‚´ìš©ê³¼ ë¡œì§ì€ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""

import os
import sys
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
import logging

# í•œê¸€ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('file_classification.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FileCategory(Enum):
    """íŒŒì¼ ì¹´í…Œê³ ë¦¬"""
    CORE = "í•µì‹¬_ì‹œìŠ¤í…œ"
    TOOLS = "ê°œë°œ_ë„êµ¬"
    DOCS = "ë¬¸ì„œ"
    TEMP = "ì„ì‹œ_íŒŒì¼"
    CONFIG = "ì„¤ì •_íŒŒì¼"
    ARCHIVE = "ë³´ê´€_íŒŒì¼"
    UNKNOWN = "ë¯¸ë¶„ë¥˜"

@dataclass
class FileInfo:
    """íŒŒì¼ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    path: str
    size: int
    modified_time: datetime
    category: FileCategory
    importance: str  # 'critical', 'important', 'normal', 'low'
    dependencies: List[str]
    description: str
    
class FileClassifier:
    """íŒŒì¼ ë¶„ë¥˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # í•µì‹¬ ì‹œìŠ¤í…œ íŒŒì¼ íŒ¨í„´ (ì ˆëŒ€ ë³´ì¡´)
        self.core_patterns = {
            'POSCO_News_250808.py': 'ë©”ì¸ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ',
            'ğŸ¹POSCO_ì›Œì¹˜í–„ìŠ¤í„°_v3_ì œì–´ì„¼í„°.bat': 'Windows ì œì–´ì„¼í„°',
            'ğŸ¹POSCO_ì›Œì¹˜í–„ìŠ¤í„°_v3_ì œì–´ì„¼í„°.command': 'macOS ì œì–´ì„¼í„°',
            'Monitoring/POSCO_News_250808/*.py': 'ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ëª¨ë“ˆ',
            'Monitoring/Posco_News_mini_v2/**/*.py': 'v2 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ'
        }
        
        # ê°œë°œ ë„êµ¬ íŒŒì¼ íŒ¨í„´
        self.tool_patterns = {
            '*_repair*.py': 'ìˆ˜ë¦¬ ë„êµ¬',
            '*_fixer*.py': 'ìˆ˜ì • ë„êµ¬',
            '*_repairer*.py': 'ë³µêµ¬ ë„êµ¬',
            'test_*.py': 'í…ŒìŠ¤íŠ¸ íŒŒì¼',
            '*_test*.py': 'í…ŒìŠ¤íŠ¸ íŒŒì¼',
            '*_testing*.py': 'í…ŒìŠ¤íŠ¸ ë„êµ¬',
            '*_verification*.py': 'ê²€ì¦ ë„êµ¬',
            '*_validator*.py': 'ê²€ì¦ ë„êµ¬',
            'automated_*.py': 'ìë™í™” ë„êµ¬',
            'enhanced_*.py': 'í–¥ìƒëœ ë„êµ¬',
            'comprehensive_*.py': 'ì¢…í•© ë„êµ¬',
            '*_cli.py': 'ëª…ë ¹í–‰ ë„êµ¬',
            '*_system.py': 'ì‹œìŠ¤í…œ ë„êµ¬'
        }
        
        # ë¬¸ì„œ íŒŒì¼ íŒ¨í„´
        self.doc_patterns = {
            '*.md': 'ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ',
            '*.txt': 'í…ìŠ¤íŠ¸ ë¬¸ì„œ',
            '*_guide*.md': 'ê°€ì´ë“œ ë¬¸ì„œ',
            '*_manual*.md': 'ë§¤ë‰´ì–¼ ë¬¸ì„œ',
            '*_documentation*.md': 'ê¸°ìˆ  ë¬¸ì„œ',
            'README*': 'README íŒŒì¼',
            '*_summary*.md': 'ìš”ì•½ ë³´ê³ ì„œ',
            '*_report*.md': 'ë³´ê³ ì„œ',
            '*_index*.md': 'ì¸ë±ìŠ¤ ë¬¸ì„œ'
        }
        
        # ì„ì‹œ íŒŒì¼ íŒ¨í„´
        self.temp_patterns = {
            'task*_completion_summary.md': 'ì‘ì—… ì™„ë£Œ ë³´ê³ ì„œ',
            'task*_*.md': 'ì‘ì—… ê´€ë ¨ ì„ì‹œ íŒŒì¼',
            '*.backup': 'ë°±ì—… íŒŒì¼',
            '*.bak': 'ë°±ì—… íŒŒì¼',
            '*_temp*': 'ì„ì‹œ íŒŒì¼',
            '*.log': 'ë¡œê·¸ íŒŒì¼',
            '*_logs*': 'ë¡œê·¸ íŒŒì¼',
            '*.tmp': 'ì„ì‹œ íŒŒì¼'
        }
        
        # ì„¤ì • íŒŒì¼ íŒ¨í„´
        self.config_patterns = {
            '*.json': 'JSON ì„¤ì • íŒŒì¼',
            '*.yaml': 'YAML ì„¤ì • íŒŒì¼',
            '*.yml': 'YAML ì„¤ì • íŒŒì¼',
            '*.ini': 'INI ì„¤ì • íŒŒì¼',
            '*.conf': 'ì„¤ì • íŒŒì¼',
            '*_config*': 'ì„¤ì • íŒŒì¼',
            '*_settings*': 'ì„¤ì • íŒŒì¼'
        }
        
        # ë³´ê´€ íŒŒì¼ íŒ¨í„´ (ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…ë“¤)
        self.archive_patterns = {
            'Task*_*.md': 'ì™„ë£Œëœ ì‘ì—… ë¬¸ì„œ',
            '*_Implementation_Summary.md': 'êµ¬í˜„ ìš”ì•½',
            '*_completion_*.md': 'ì™„ë£Œ ë³´ê³ ì„œ',
            'ModuleRegistry_*.md': 'ëª¨ë“ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬',
            'migration_*.py': 'ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸',
            'post_migration_*.py': 'ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ì²˜ë¦¬'
        }
        
        self.classification_results = {}
        self.duplicate_files = []
        self.large_files = []
        
    def classify_all_files(self) -> Dict[FileCategory, List[FileInfo]]:
        """ëª¨ë“  íŒŒì¼ ë¶„ë¥˜"""
        logger.info("ğŸ“‚ ì „ì²´ íŒŒì¼ ë¶„ë¥˜ ì‹œì‘")
        
        classified_files = {category: [] for category in FileCategory}
        total_files = 0
        total_size = 0
        
        # ì œì™¸í•  ë””ë ‰í† ë¦¬
        exclude_dirs = {
            '__pycache__',
            '.git',
            '.kiro',
            'node_modules',
            '.vscode',
            '.idea'
        }
        
        for root, dirs, files in os.walk('.'):
            # ì œì™¸ ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                file_path = Path(root) / file
                
                try:
                    # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
                    stat_info = file_path.stat()
                    file_info = FileInfo(
                        path=str(file_path),
                        size=stat_info.st_size,
                        modified_time=datetime.fromtimestamp(stat_info.st_mtime),
                        category=self._classify_file(file_path),
                        importance=self._determine_importance(file_path),
                        dependencies=self._analyze_dependencies(file_path),
                        description=self._get_file_description(file_path)
                    )
                    
                    classified_files[file_info.category].append(file_info)
                    total_files += 1
                    total_size += file_info.size
                    
                    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì‹ë³„ (10MB ì´ìƒ)
                    if file_info.size > 10 * 1024 * 1024:
                        self.large_files.append(file_info)
                    
                    if total_files % 500 == 0:
                        logger.info(f"ì§„í–‰ ìƒí™©: {total_files}ê°œ íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ")
                        
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨ {file_path}: {e}")
        
        # ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬
        self._find_duplicate_files(classified_files)
        
        # ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
        self.classification_results = classified_files
        
        logger.info(f"âœ… íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ")
        logger.info(f"   ì´ íŒŒì¼ ìˆ˜: {total_files:,}ê°œ")
        logger.info(f"   ì´ í¬ê¸°: {total_size / 1024 / 1024:.1f}MB")
        logger.info(f"   ëŒ€ìš©ëŸ‰ íŒŒì¼: {len(self.large_files)}ê°œ")
        logger.info(f"   ì¤‘ë³µ íŒŒì¼: {len(self.duplicate_files)}ê°œ")
        
        return classified_files
    
    def _classify_file(self, file_path: Path) -> FileCategory:
        """ê°œë³„ íŒŒì¼ ë¶„ë¥˜"""
        file_str = str(file_path)
        file_name = file_path.name
        
        # í•µì‹¬ ì‹œìŠ¤í…œ íŒŒì¼ í™•ì¸
        for pattern, desc in self.core_patterns.items():
            if self._match_pattern(file_str, pattern):
                return FileCategory.CORE
        
        # ë³´ê´€ íŒŒì¼ í™•ì¸ (ì„ì‹œ íŒŒì¼ë³´ë‹¤ ìš°ì„ )
        for pattern, desc in self.archive_patterns.items():
            if self._match_pattern(file_name, pattern):
                return FileCategory.ARCHIVE
        
        # ì„ì‹œ íŒŒì¼ í™•ì¸
        for pattern, desc in self.temp_patterns.items():
            if self._match_pattern(file_name, pattern):
                return FileCategory.TEMP
        
        # ê°œë°œ ë„êµ¬ í™•ì¸
        for pattern, desc in self.tool_patterns.items():
            if self._match_pattern(file_name, pattern):
                return FileCategory.TOOLS
        
        # ì„¤ì • íŒŒì¼ í™•ì¸
        for pattern, desc in self.config_patterns.items():
            if self._match_pattern(file_name, pattern):
                return FileCategory.CONFIG
        
        # ë¬¸ì„œ íŒŒì¼ í™•ì¸
        for pattern, desc in self.doc_patterns.items():
            if self._match_pattern(file_name, pattern):
                return FileCategory.DOCS
        
        return FileCategory.UNKNOWN
    
    def _match_pattern(self, file_path: str, pattern: str) -> bool:
        """íŒ¨í„´ ë§¤ì¹­"""
        import fnmatch
        
        # ê²½ë¡œ íŒ¨í„´ ì²˜ë¦¬
        if '/' in pattern:
            return fnmatch.fnmatch(file_path, pattern)
        else:
            # íŒŒì¼ëª…ë§Œ ë§¤ì¹­
            file_name = Path(file_path).name
            return fnmatch.fnmatch(file_name, pattern)
    
    def _determine_importance(self, file_path: Path) -> str:
        """íŒŒì¼ ì¤‘ìš”ë„ ê²°ì •"""
        file_str = str(file_path)
        file_name = file_path.name
        
        # í•µì‹¬ íŒŒì¼
        critical_patterns = [
            'POSCO_News_250808.py',
            'ğŸ¹POSCO_ì›Œì¹˜í–„ìŠ¤í„°_v3_ì œì–´ì„¼í„°.*',
            'Monitoring/POSCO_News_250808/*.py'
        ]
        
        for pattern in critical_patterns:
            if self._match_pattern(file_str, pattern):
                return 'critical'
        
        # ì¤‘ìš” íŒŒì¼
        important_patterns = [
            '*_system.py',
            '*_manager.py',
            '*_verification.py',
            'final_*.py'
        ]
        
        for pattern in important_patterns:
            if self._match_pattern(file_name, pattern):
                return 'important'
        
        # ì„ì‹œ íŒŒì¼ì€ ë‚®ì€ ì¤‘ìš”ë„
        if file_name.startswith('task') or 'temp' in file_name.lower():
            return 'low'
        
        return 'normal'
    
    def _analyze_dependencies(self, file_path: Path) -> List[str]:
        """íŒŒì¼ ì˜ì¡´ì„± ë¶„ì„"""
        dependencies = []
        
        if file_path.suffix == '.py':
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # import êµ¬ë¬¸ ì°¾ê¸°
                import re
                import_patterns = [
                    r'from\s+([^\s]+)\s+import',
                    r'import\s+([^\s,]+)'
                ]
                
                for pattern in import_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        if not match.startswith('.') and match not in ['os', 'sys', 'json', 'time']:
                            dependencies.append(match)
                
            except Exception:
                pass
        
        return list(set(dependencies))  # ì¤‘ë³µ ì œê±°
    
    def _get_file_description(self, file_path: Path) -> str:
        """íŒŒì¼ ì„¤ëª… ìƒì„±"""
        file_str = str(file_path)
        file_name = file_path.name
        
        # í•µì‹¬ íŒŒì¼ ì„¤ëª…
        for pattern, desc in self.core_patterns.items():
            if self._match_pattern(file_str, pattern):
                return desc
        
        # ë„êµ¬ íŒŒì¼ ì„¤ëª…
        for pattern, desc in self.tool_patterns.items():
            if self._match_pattern(file_name, pattern):
                return desc
        
        # ë¬¸ì„œ íŒŒì¼ ì„¤ëª…
        for pattern, desc in self.doc_patterns.items():
            if self._match_pattern(file_name, pattern):
                return desc
        
        # ì„ì‹œ íŒŒì¼ ì„¤ëª…
        for pattern, desc in self.temp_patterns.items():
            if self._match_pattern(file_name, pattern):
                return desc
        
        # ì„¤ì • íŒŒì¼ ì„¤ëª…
        for pattern, desc in self.config_patterns.items():
            if self._match_pattern(file_name, pattern):
                return desc
        
        # ë³´ê´€ íŒŒì¼ ì„¤ëª…
        for pattern, desc in self.archive_patterns.items():
            if self._match_pattern(file_name, pattern):
                return desc
        
        return f"{file_path.suffix} íŒŒì¼"
    
    def _find_duplicate_files(self, classified_files: Dict[FileCategory, List[FileInfo]]):
        """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°"""
        logger.info("ğŸ” ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬ ì¤‘...")
        
        file_hashes = {}
        
        for category, files in classified_files.items():
            for file_info in files:
                try:
                    # ì‘ì€ íŒŒì¼ë§Œ í•´ì‹œ ê³„ì‚° (ì„±ëŠ¥ìƒ ì´ìœ )
                    if file_info.size < 1024 * 1024:  # 1MB ë¯¸ë§Œ
                        with open(file_info.path, 'rb') as f:
                            content = f.read()
                            file_hash = hashlib.md5(content).hexdigest()
                            
                            if file_hash in file_hashes:
                                self.duplicate_files.append({
                                    'original': file_hashes[file_hash],
                                    'duplicate': file_info.path,
                                    'size': file_info.size
                                })
                            else:
                                file_hashes[file_hash] = file_info.path
                                
                except Exception:
                    continue
        
        logger.info(f"ì¤‘ë³µ íŒŒì¼ {len(self.duplicate_files)}ê°œ ë°œê²¬")
    
    def generate_classification_report(self) -> str:
        """ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±"""
        if not self.classification_results:
            return "ë¶„ë¥˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € classify_all_files()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        
        report_time = datetime.now()
        
        report = f"""
# POSCO ì‹œìŠ¤í…œ íŒŒì¼ ë¶„ë¥˜ ë³´ê³ ì„œ

**ìƒì„± ì‹œê°„**: {report_time.strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½

"""
        
        total_files = sum(len(files) for files in self.classification_results.values())
        total_size = sum(sum(f.size for f in files) for files in self.classification_results.values())
        
        report += f"- **ì´ íŒŒì¼ ìˆ˜**: {total_files:,}ê°œ\n"
        report += f"- **ì´ í¬ê¸°**: {total_size / 1024 / 1024:.1f}MB\n"
        report += f"- **ëŒ€ìš©ëŸ‰ íŒŒì¼**: {len(self.large_files)}ê°œ\n"
        report += f"- **ì¤‘ë³µ íŒŒì¼**: {len(self.duplicate_files)}ê°œ\n\n"
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        report += "## ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ê²°ê³¼\n\n"
        
        for category, files in self.classification_results.items():
            if not files:
                continue
                
            category_size = sum(f.size for f in files)
            report += f"### {category.value}\n\n"
            report += f"- **íŒŒì¼ ìˆ˜**: {len(files):,}ê°œ\n"
            report += f"- **ì´ í¬ê¸°**: {category_size / 1024 / 1024:.1f}MB\n"
            report += f"- **í‰ê·  í¬ê¸°**: {category_size / len(files) / 1024:.1f}KB\n"
            
            # ì¤‘ìš”ë„ë³„ ë¶„ë¥˜
            importance_count = {}
            for file_info in files:
                importance_count[file_info.importance] = importance_count.get(file_info.importance, 0) + 1
            
            if importance_count:
                report += "- **ì¤‘ìš”ë„ë³„**:\n"
                for importance, count in sorted(importance_count.items()):
                    report += f"  - {importance}: {count}ê°œ\n"
            
            # ìƒìœ„ 5ê°œ íŒŒì¼ (í¬ê¸°ìˆœ)
            top_files = sorted(files, key=lambda x: x.size, reverse=True)[:5]
            if top_files:
                report += "- **ì£¼ìš” íŒŒì¼** (í¬ê¸°ìˆœ):\n"
                for file_info in top_files:
                    size_mb = file_info.size / 1024 / 1024
                    report += f"  - `{file_info.path}` ({size_mb:.1f}MB) - {file_info.description}\n"
            
            report += "\n"
        
        # ì¤‘ë³µ íŒŒì¼ ì •ë³´
        if self.duplicate_files:
            report += "## ğŸ”„ ì¤‘ë³µ íŒŒì¼ ëª©ë¡\n\n"
            for dup in self.duplicate_files[:10]:  # ìƒìœ„ 10ê°œë§Œ
                size_kb = dup['size'] / 1024
                report += f"- `{dup['original']}` â†” `{dup['duplicate']}` ({size_kb:.1f}KB)\n"
            
            if len(self.duplicate_files) > 10:
                report += f"- ... ë° {len(self.duplicate_files) - 10}ê°œ ì¶”ê°€\n"
            report += "\n"
        
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì •ë³´
        if self.large_files:
            report += "## ğŸ“¦ ëŒ€ìš©ëŸ‰ íŒŒì¼ ëª©ë¡\n\n"
            for file_info in sorted(self.large_files, key=lambda x: x.size, reverse=True):
                size_mb = file_info.size / 1024 / 1024
                report += f"- `{file_info.path}` ({size_mb:.1f}MB) - {file_info.description}\n"
            report += "\n"
        
        # ì •ë¦¬ ê¶Œì¥ì‚¬í•­
        report += "## ğŸ’¡ ì •ë¦¬ ê¶Œì¥ì‚¬í•­\n\n"
        
        temp_files = self.classification_results.get(FileCategory.TEMP, [])
        archive_files = self.classification_results.get(FileCategory.ARCHIVE, [])
        
        if temp_files:
            temp_size = sum(f.size for f in temp_files) / 1024 / 1024
            report += f"1. **ì„ì‹œ íŒŒì¼ ì •ë¦¬**: {len(temp_files)}ê°œ íŒŒì¼ ({temp_size:.1f}MB) â†’ `archive/temp/` ì´ë™\n"
        
        if archive_files:
            archive_size = sum(f.size for f in archive_files) / 1024 / 1024
            report += f"2. **ì™„ë£Œ ì‘ì—… ë³´ê´€**: {len(archive_files)}ê°œ íŒŒì¼ ({archive_size:.1f}MB) â†’ `archive/completed/` ì´ë™\n"
        
        if self.duplicate_files:
            dup_size = sum(dup['size'] for dup in self.duplicate_files) / 1024 / 1024
            report += f"3. **ì¤‘ë³µ íŒŒì¼ ì œê±°**: {len(self.duplicate_files)}ê°œ íŒŒì¼ ({dup_size:.1f}MB) ì ˆì•½ ê°€ëŠ¥\n"
        
        if self.large_files:
            report += f"4. **ëŒ€ìš©ëŸ‰ íŒŒì¼ ì••ì¶•**: {len(self.large_files)}ê°œ íŒŒì¼ ì••ì¶• ê²€í† \n"
        
        # ì˜ˆìƒ ì ˆì•½ íš¨ê³¼
        potential_savings = 0
        if temp_files:
            potential_savings += sum(f.size for f in temp_files)
        if self.duplicate_files:
            potential_savings += sum(dup['size'] for dup in self.duplicate_files)
        
        if potential_savings > 0:
            savings_mb = potential_savings / 1024 / 1024
            savings_percent = (potential_savings / total_size) * 100
            report += f"\n**ì˜ˆìƒ ì ˆì•½ íš¨ê³¼**: {savings_mb:.1f}MB ({savings_percent:.1f}%)\n"
        
        report += f"""
---
*ì´ ë³´ê³ ì„œëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        
        return report
    
    def save_classification_data(self, output_file: str = "file_classification_data.json"):
        """ë¶„ë¥˜ ë°ì´í„° JSONìœ¼ë¡œ ì €ì¥"""
        if not self.classification_results:
            logger.warning("ì €ì¥í•  ë¶„ë¥˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_data = {}
        
        for category, files in self.classification_results.items():
            serializable_data[category.value] = []
            for file_info in files:
                serializable_data[category.value].append({
                    'path': file_info.path,
                    'size': file_info.size,
                    'modified_time': file_info.modified_time.isoformat(),
                    'importance': file_info.importance,
                    'dependencies': file_info.dependencies,
                    'description': file_info.description
                })
        
        # ì¶”ê°€ ì •ë³´
        serializable_data['_metadata'] = {
            'classification_time': datetime.now().isoformat(),
            'total_files': sum(len(files) for files in self.classification_results.values()),
            'duplicate_files': self.duplicate_files,
            'large_files': [
                {
                    'path': f.path,
                    'size': f.size,
                    'description': f.description
                } for f in self.large_files
            ]
        }
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ ë¶„ë¥˜ ë°ì´í„° ì €ì¥: {output_file}")
            
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='POSCO ì‹œìŠ¤í…œ íŒŒì¼ ë¶„ë¥˜')
    parser.add_argument('--classify', action='store_true', help='ì „ì²´ íŒŒì¼ ë¶„ë¥˜ ì‹¤í–‰')
    parser.add_argument('--report', action='store_true', help='ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±')
    parser.add_argument('--save-data', action='store_true', help='ë¶„ë¥˜ ë°ì´í„° JSON ì €ì¥')
    parser.add_argument('--category', type=str, help='íŠ¹ì • ì¹´í…Œê³ ë¦¬ íŒŒì¼ ëª©ë¡ ì¶œë ¥')
    
    args = parser.parse_args()
    
    classifier = FileClassifier()
    
    try:
        if args.classify:
            logger.info("ğŸš€ íŒŒì¼ ë¶„ë¥˜ ì‹œì‘")
            classified_files = classifier.classify_all_files()
            
            # ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            logger.info("\nğŸ“Š ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½:")
            for category, files in classified_files.items():
                if files:
                    size_mb = sum(f.size for f in files) / 1024 / 1024
                    logger.info(f"  {category.value}: {len(files):,}ê°œ ({size_mb:.1f}MB)")
        
        if args.report:
            logger.info("ğŸ“‹ ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
            report = classifier.generate_classification_report()
            
            report_file = f"file_classification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"âœ… ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±: {report_file}")
        
        if args.save_data:
            classifier.save_classification_data()
        
        if args.category:
            try:
                category = FileCategory(args.category)
                files = classifier.classification_results.get(category, [])
                
                logger.info(f"\nğŸ“‚ {category.value} ì¹´í…Œê³ ë¦¬ íŒŒì¼ ëª©ë¡:")
                for file_info in files:
                    size_kb = file_info.size / 1024
                    logger.info(f"  - {file_info.path} ({size_kb:.1f}KB) - {file_info.description}")
                    
            except ValueError:
                logger.error(f"ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬: {args.category}")
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: " + ", ".join([c.value for c in FileCategory]))
        
        if not any([args.classify, args.report, args.save_data, args.category]):
            parser.print_help()
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()