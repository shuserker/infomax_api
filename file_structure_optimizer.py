#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
POSCO ì‹œìŠ¤í…œ íŒŒì¼ êµ¬ì¡° ì •ë¦¬ ë° ìµœì í™” ë„êµ¬
í˜„ì¬ 4,602ê°œ íŒŒì¼ì„ 1,743ê°œë¡œ ìµœì í™”
"""

import os
import shutil
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime
import hashlib

class FileStructureOptimizer:
    def __init__(self, workspace_root: str = "."):
        self.workspace_root = Path(workspace_root)
        self.backup_folders = [
            ".aggressive_syntax_repair_backup",
            ".comprehensive_repair_backup", 
            ".enhanced_repair_backups",
            ".file_reference_backup",
            ".file_renaming_backup",
            ".filename_standardization_backup",
            ".final_file_reference_cleanup_backup",
            ".final_reference_cleanup_backup",
            ".final_syntax_repair_backup",
            ".focused_file_reference_backup",
            ".indentation_backup",
            ".naming_backup",
            ".refined_file_reference_backup",
            ".repair_backups",
            ".syntax_repair_backup"
        ]
        
        self.temp_folders = [
            "__pycache__",
            "cache",
            "logs",
            "migration_logs",
            "migration_reports",
            "analysis_reports",
            "reports",
            "webhook_backup",
            "deployment_backup_20250810_185935"
        ]
        
        self.duplicate_patterns = [
            r".*\.backup.*",
            r".*\.old$",
            r".*_old\.py$",
            r".*\.backup_emergency$",
            r".*_backup_\d+.*"
        ]
        
        self.optimization_report = {
            "start_time": datetime.now().isoformat(),
            "initial_file_count": 0,
            "final_file_count": 0,
            "removed_folders": [],
            "removed_files": [],
            "renamed_files": [],
            "duplicate_files": [],
            "optimization_summary": {}
        }
    
    def analyze_current_structure(self) -> Dict:
        """í˜„ì¬ íŒŒì¼ êµ¬ì¡° ë¶„ì„"""
        print("ğŸ“Š í˜„ì¬ íŒŒì¼ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        analysis = {
            "total_files": 0,
            "total_folders": 0,
            "backup_folders": [],
            "temp_folders": [],
            "duplicate_files": [],
            "large_files": [],
            "file_types": {},
            "folder_sizes": {}
        }
        
        for root, dirs, files in os.walk(self.workspace_root):
            root_path = Path(root)
            relative_root = root_path.relative_to(self.workspace_root)
            
            # í´ë” ë¶„ì„
            analysis["total_folders"] += len(dirs)
            
            # ë°±ì—… í´ë” ì‹ë³„
            for folder in dirs:
                if folder in self.backup_folders:
                    analysis["backup_folders"].append(str(relative_root / folder))
                elif folder in self.temp_folders:
                    analysis["temp_folders"].append(str(relative_root / folder))
            
            # íŒŒì¼ ë¶„ì„
            for file in files:
                file_path = root_path / file
                relative_path = file_path.relative_to(self.workspace_root)
                
                analysis["total_files"] += 1
                
                # íŒŒì¼ í™•ì¥ìë³„ ë¶„ë¥˜
                ext = file_path.suffix.lower()
                analysis["file_types"][ext] = analysis["file_types"].get(ext, 0) + 1
                
                # í° íŒŒì¼ ì‹ë³„ (10MB ì´ìƒ)
                try:
                    size = file_path.stat().st_size
                    if size > 10 * 1024 * 1024:  # 10MB
                        analysis["large_files"].append({
                            "path": str(relative_path),
                            "size_mb": round(size / (1024 * 1024), 2)
                        })
                except:
                    pass
                
                # ì¤‘ë³µ íŒŒì¼ íŒ¨í„´ í™•ì¸
                for pattern in self.duplicate_patterns:
                    if re.match(pattern, file):
                        analysis["duplicate_files"].append(str(relative_path))
                        break
        
        self.optimization_report["initial_file_count"] = analysis["total_files"]
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
        print(f"   - ì´ íŒŒì¼ ìˆ˜: {analysis['total_files']:,}ê°œ")
        print(f"   - ì´ í´ë” ìˆ˜: {analysis['total_folders']:,}ê°œ")
        print(f"   - ë°±ì—… í´ë”: {len(analysis['backup_folders'])}ê°œ")
        print(f"   - ì„ì‹œ í´ë”: {len(analysis['temp_folders'])}ê°œ")
        print(f"   - ì¤‘ë³µ íŒŒì¼: {len(analysis['duplicate_files'])}ê°œ")
        
        return analysis
    
    def remove_backup_folders(self) -> List[str]:
        """ë°±ì—… í´ë”ë“¤ ì œê±°"""
        print("ğŸ—‚ï¸ ë¶ˆí•„ìš”í•œ ë°±ì—… í´ë”ë“¤ì„ ì œê±°í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        removed_folders = []
        
        for folder_name in self.backup_folders:
            folder_path = self.workspace_root / folder_name
            if folder_path.exists() and folder_path.is_dir():
                try:
                    # í´ë” í¬ê¸° ê³„ì‚°
                    total_size = sum(f.stat().st_size for f in folder_path.rglob('*') if f.is_file())
                    size_mb = round(total_size / (1024 * 1024), 2)
                    
                    shutil.rmtree(folder_path)
                    removed_folders.append(f"{folder_name} ({size_mb}MB)")
                    print(f"   âœ… ì œê±°ë¨: {folder_name} ({size_mb}MB)")
                except Exception as e:
                    print(f"   âŒ ì œê±° ì‹¤íŒ¨: {folder_name} - {e}")
        
        self.optimization_report["removed_folders"] = removed_folders
        return removed_folders
    
    def remove_temp_folders(self) -> List[str]:
        """ì„ì‹œ í´ë”ë“¤ ì œê±°"""
        print("ğŸ“ ì„ì‹œ í´ë”ë“¤ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        removed_temp = []
        
        for folder_name in self.temp_folders:
            for folder_path in self.workspace_root.rglob(folder_name):
                if folder_path.is_dir():
                    try:
                        # ì¤‘ìš”í•œ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                        important_files = []
                        for file_path in folder_path.rglob('*'):
                            if file_path.is_file() and file_path.suffix in ['.py', '.md', '.json', '.yaml', '.yml']:
                                # ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼ì¸ì§€ í™•ì¸
                                mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                                if (datetime.now() - mtime).days < 7:
                                    important_files.append(str(file_path))
                        
                        if not important_files:
                            total_size = sum(f.stat().st_size for f in folder_path.rglob('*') if f.is_file())
                            size_mb = round(total_size / (1024 * 1024), 2)
                            
                            shutil.rmtree(folder_path)
                            removed_temp.append(f"{folder_path.relative_to(self.workspace_root)} ({size_mb}MB)")
                            print(f"   âœ… ì œê±°ë¨: {folder_path.relative_to(self.workspace_root)} ({size_mb}MB)")
                        else:
                            print(f"   âš ï¸ ë³´ì¡´ë¨: {folder_path.relative_to(self.workspace_root)} (ì¤‘ìš” íŒŒì¼ {len(important_files)}ê°œ)")
                    except Exception as e:
                        print(f"   âŒ ì œê±° ì‹¤íŒ¨: {folder_path} - {e}")
        
        return removed_temp
    
    def identify_duplicate_files(self) -> List[Dict]:
        """ì¤‘ë³µ íŒŒì¼ ì‹ë³„"""
        print("ğŸ” ì¤‘ë³µ íŒŒì¼ì„ ì‹ë³„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        file_hashes = {}
        duplicates = []
        
        for file_path in self.workspace_root.rglob('*'):
            if file_path.is_file() and not any(part.startswith('.') for part in file_path.parts[1:]):
                try:
                    # íŒŒì¼ í•´ì‹œ ê³„ì‚°
                    with open(file_path, 'rb') as f:
                        file_hash = hashlib.md5(f.read()).hexdigest()
                    
                    relative_path = file_path.relative_to(self.workspace_root)
                    
                    if file_hash in file_hashes:
                        duplicates.append({
                            "original": file_hashes[file_hash],
                            "duplicate": str(relative_path),
                            "hash": file_hash
                        })
                    else:
                        file_hashes[file_hash] = str(relative_path)
                        
                except Exception as e:
                    continue
        
        print(f"   ğŸ“‹ ì¤‘ë³µ íŒŒì¼ {len(duplicates)}ê°œ ë°œê²¬")
        self.optimization_report["duplicate_files"] = duplicates
        return duplicates
    
    def remove_duplicate_files(self, duplicates: List[Dict]) -> List[str]:
        """ì¤‘ë³µ íŒŒì¼ ì œê±°"""
        print("ğŸ—‘ï¸ ì¤‘ë³µ íŒŒì¼ì„ ì œê±°í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        removed_files = []
        
        for dup in duplicates:
            duplicate_path = self.workspace_root / dup["duplicate"]
            
            # ë°±ì—… íŒŒì¼ì´ë‚˜ ì„ì‹œ íŒŒì¼ ìš°ì„  ì œê±°
            if any(pattern in dup["duplicate"] for pattern in [".backup", "_old", ".emergency"]):
                try:
                    duplicate_path.unlink()
                    removed_files.append(dup["duplicate"])
                    print(f"   âœ… ì œê±°ë¨: {dup['duplicate']}")
                except Exception as e:
                    print(f"   âŒ ì œê±° ì‹¤íŒ¨: {dup['duplicate']} - {e}")
        
        self.optimization_report["removed_files"] = removed_files
        return removed_files
    
    def standardize_filenames(self) -> List[Dict]:
        """íŒŒì¼ëª… í‘œì¤€í™”"""
        print("ğŸ“ íŒŒì¼ëª…ì„ í‘œì¤€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        renamed_files = []
        
        # í•œêµ­ì–´/ì˜ì–´ íŒŒì¼ëª… ê·œì¹™
        korean_patterns = {
            r"ì›Œì¹˜í–„ìŠ¤í„°": "watchhamster",
            r"ì œì–´ì„¼í„°": "control_center", 
            r"ì•Œë¦¼": "notification",
            r"ëª¨ë‹ˆí„°ë§": "monitoring",
            r"ì‹œìŠ¤í…œ": "system",
            r"í…ŒìŠ¤íŠ¸": "test",
            r"ì„¤ì •": "config",
            r"ë³´ê³ ì„œ": "report"
        }
        
        for file_path in self.workspace_root.rglob('*'):
            if file_path.is_file():
                original_name = file_path.name
                new_name = original_name
                
                # í•œêµ­ì–´ â†’ ì˜ì–´ ë³€í™˜
                for korean, english in korean_patterns.items():
                    if korean in new_name:
                        new_name = re.sub(korean, english, new_name)
                
                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                new_name = re.sub(r'[ğŸ›ï¸ğŸ¹ğŸš€ğŸ”„ğŸ”ğŸ””ğŸ”§ğŸ—‚ï¸ğŸ› ï¸ğŸ“‹ğŸ¨]', '', new_name)
                new_name = re.sub(r'[^\w\-_.]', '_', new_name)
                new_name = re.sub(r'_+', '_', new_name)
                new_name = new_name.strip('_')
                
                if new_name != original_name and new_name:
                    new_path = file_path.parent / new_name
                    if not new_path.exists():
                        try:
                            file_path.rename(new_path)
                            renamed_files.append({
                                "original": original_name,
                                "new": new_name,
                                "path": str(file_path.parent.relative_to(self.workspace_root))
                            })
                            print(f"   âœ… ì´ë¦„ ë³€ê²½: {original_name} â†’ {new_name}")
                        except Exception as e:
                            print(f"   âŒ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {original_name} - {e}")
        
        self.optimization_report["renamed_files"] = renamed_files
        return renamed_files
    
    def optimize_folder_structure(self) -> Dict:
        """í´ë” êµ¬ì¡° ìµœì í™”"""
        print("ğŸ“‚ í´ë” êµ¬ì¡°ë¥¼ ìµœì í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # í•µì‹¬ í´ë” êµ¬ì¡° ì •ì˜
        core_structure = {
            "core": "í•µì‹¬ ì‹œìŠ¤í…œ ëª¨ë“ˆ",
            "config": "ì„¤ì • íŒŒì¼",
            "scripts": "ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸",
            "tools": "ìœ í‹¸ë¦¬í‹° ë„êµ¬",
            "docs": "ë¬¸ì„œ",
            "recovery_config": "ë³µêµ¬ ì„¤ì •",
            ".kiro": "Kiro ì„¤ì •"
        }
        
        optimization_summary = {
            "preserved_folders": list(core_structure.keys()),
            "created_folders": [],
            "moved_files": []
        }
        
        # í•„ìš”í•œ í´ë” ìƒì„±
        for folder_name in core_structure.keys():
            folder_path = self.workspace_root / folder_name
            if not folder_path.exists():
                folder_path.mkdir(exist_ok=True)
                optimization_summary["created_folders"].append(folder_name)
                print(f"   âœ… í´ë” ìƒì„±: {folder_name}")
        
        return optimization_summary
    
    def generate_optimization_report(self) -> str:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“Š ìµœì í™” ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ìµœì¢… íŒŒì¼ ìˆ˜ ê³„ì‚°
        final_count = sum(1 for _ in self.workspace_root.rglob('*') if _.is_file())
        self.optimization_report["final_file_count"] = final_count
        self.optimization_report["end_time"] = datetime.now().isoformat()
        
        # ìµœì í™” ìš”ì•½
        removed_count = len(self.optimization_report["removed_files"])
        renamed_count = len(self.optimization_report["renamed_files"])
        folder_count = len(self.optimization_report["removed_folders"])
        
        self.optimization_report["optimization_summary"] = {
            "files_removed": removed_count,
            "files_renamed": renamed_count,
            "folders_removed": folder_count,
            "size_reduction": self.optimization_report["initial_file_count"] - final_count,
            "optimization_percentage": round((self.optimization_report["initial_file_count"] - final_count) / self.optimization_report["initial_file_count"] * 100, 2)
        }
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_path = self.workspace_root / "file_structure_optimization_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.optimization_report, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        summary = f"""
# íŒŒì¼ êµ¬ì¡° ìµœì í™” ì™„ë£Œ ë³´ê³ ì„œ

## ìµœì í™” ê²°ê³¼
- **ì‹œì‘ íŒŒì¼ ìˆ˜**: {self.optimization_report['initial_file_count']:,}ê°œ
- **ìµœì¢… íŒŒì¼ ìˆ˜**: {final_count:,}ê°œ
- **ì œê±°ëœ íŒŒì¼**: {removed_count:,}ê°œ
- **ìµœì í™”ìœ¨**: {self.optimization_report['optimization_summary']['optimization_percentage']}%

## ì œê±°ëœ ë°±ì—… í´ë”
{chr(10).join(f"- {folder}" for folder in self.optimization_report['removed_folders'])}

## íŒŒì¼ëª… í‘œì¤€í™”
- ì´ë¦„ ë³€ê²½ëœ íŒŒì¼: {renamed_count}ê°œ
- í•œêµ­ì–´ â†’ ì˜ì–´ ë³€í™˜ ì ìš©
- íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ ì™„ë£Œ

## ëª©í‘œ ë‹¬ì„±ë„
- ëª©í‘œ: 1,743ê°œ íŒŒì¼
- í˜„ì¬: {final_count:,}ê°œ íŒŒì¼
- ëª©í‘œ ëŒ€ë¹„: {'âœ… ë‹¬ì„±' if final_count <= 1743 else f'âŒ ì´ˆê³¼ ({final_count - 1743}ê°œ)'}

ìµœì í™” ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
"""
        
        summary_path = self.workspace_root / "optimization_summary.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        print("âœ… ìµœì í™” ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"   - ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
        print(f"   - ìš”ì•½ ë³´ê³ ì„œ: {summary_path}")
        
        return summary

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ POSCO ì‹œìŠ¤í…œ íŒŒì¼ êµ¬ì¡° ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    optimizer = FileStructureOptimizer()
    
    try:
        # 1. í˜„ì¬ êµ¬ì¡° ë¶„ì„
        analysis = optimizer.analyze_current_structure()
        
        # 2. ë°±ì—… í´ë” ì œê±°
        removed_backups = optimizer.remove_backup_folders()
        
        # 3. ì„ì‹œ í´ë” ì •ë¦¬
        removed_temps = optimizer.remove_temp_folders()
        
        # 4. ì¤‘ë³µ íŒŒì¼ ì‹ë³„ ë° ì œê±°
        duplicates = optimizer.identify_duplicate_files()
        removed_duplicates = optimizer.remove_duplicate_files(duplicates)
        
        # 5. íŒŒì¼ëª… í‘œì¤€í™”
        renamed_files = optimizer.standardize_filenames()
        
        # 6. í´ë” êµ¬ì¡° ìµœì í™”
        structure_optimization = optimizer.optimize_folder_structure()
        
        # 7. ìµœì í™” ë³´ê³ ì„œ ìƒì„±
        summary = optimizer.generate_optimization_report()
        
        print("=" * 60)
        print("ğŸ‰ íŒŒì¼ êµ¬ì¡° ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(summary)
        
    except Exception as e:
        print(f"âŒ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    return True

if __name__ == "__main__":
    main()