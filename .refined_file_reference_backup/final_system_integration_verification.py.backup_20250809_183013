#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Final System Integration Verification
POSCO 시스템 구성요소

WatchHamster v3.0 및 POSCO News 250808 호환
Created: 2025-08-08
"""

import posco_news_250808_monitor.log
import system_functionality_verification.py
# BROKEN_REF: import subprocess
import .comprehensive_repair_backup/realtime_news_monitor.py.backup_20250809_181657
import test_config.json
# BROKEN_REF: from datetime import datetime
# BROKEN_REF: from typing import Dict, List, Tuple, Optional

class FinalSystemIntegrationVerification:
    """최종 시스템 통합 및 검증"""
    
    def __init__(self):
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.verification_results = {}
        self.start_time = None
        self.end_time = None
        
        # 검증 단계 정의
        self.verification_stages = [
            {
                'name': 'comprehensive_test_suite',
                'title': '완전한 테스트 스위트 실행',
                'description': '모든 테스트 스크립트 실행 및 통합 문제 수정',
                'critical': True
            },
            {
                'name': 'control_center_verification',
                'title': '제어센터 메뉴 기능 검증',
                'description': '모든 제어센터 메뉴가 올바르게 작동하는지 검증',
                'critical': True
            },
            {
                'name': 'performance_benchmarking',
                'title': '최종 성능 벤치마킹',
                'description': '성능 벤치마킹 및 최적화 수행',
                'critical': False
            },
            {
                'name': 'deployment_checklist',
                'title': '배포 검증 체크리스트',
                'description': '배포 준비 상태 확인 및 문서화',
                'critical': True
            }
        ]
    
    def print_header(self, title: str, char: str = "=", width: int = 80):
        """헤더 출력"""
        print(char * width)
        print(f"{title:^{width}}")
        print(char * width)
    
    def print_section(self, title: str, char: str = "-", width: int = 60):
        """섹션 헤더 출력"""
        print(f"/n{char * width}")
        print(f"{title}")
        print(f"{char * width}") 
   
    def run_comprehensive_test_suite(self) -> Dict:
        """완전한 테스트 스위트 실행"""
        print("🧪 완전한 테스트 스위트 실행 중...")
        
        result = {
            'stage': 'comprehensive_test_suite',
            'success': False,
            'details': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # 1. 종합 테스트 실행
            print("  📋 종합 테스트 프레임워크 실행...")
            
            if os.path.exists('run_comprehensive_tests.py'):
                test_result = subprocess.run([
                    sys.executable, 'run_comprehensive_tests.py'
],_capture_output = True, text=True, timeout=600)
                
                result['details']['comprehensive_tests'] = {
                    'return_code': test_result.returncode,
                    'stdout': test_result.stdout,
                    'stderr': test_result.stderr
                }
                
                if test_result.returncode == 0:
                    print("    ✅ 종합 테스트 성공")
                    result['details']['comprehensive_tests']['status'] = 'passed'
                else:
                    print("    ❌ 종합 테스트 실패")
                    result['details']['comprehensive_tests']['status'] = 'failed'
                    result['issues'].append("종합 테스트 실패")
            else:
                print("    ⚠️ 종합 테스트 스크립트 없음")
                result['issues'].append("종합 테스트 스크립트 누락")
            
            # 2. 개별 테스트 스크립트 실행
            individual_tests = [
# BROKEN_REF:                 'test_v2_integration.py',
                'test_process_lifecycle.py', 
                'test_control_center_integration.py',
                'test_end_to_end_integration.py'
            ]
            
            result['details']['individual_tests'] = {}
            passed_tests = 0
            
            for test_script in individual_tests:
                print(f"  🧪 {test_script} 실행 중...")
                
                if os.path.exists(test_script):
                    try:
                        test_result = subprocess.run([
                            sys.executable, test_script
],_capture_output = True, text=True, timeout=120)
                        
                        test_status = 'passed' if test_result.returncode == 0 else 'failed'
                        result['details']['individual_tests'][test_script] = {
                            'status': test_status,
                            'return_code': test_result.returncode,
                            'stdout': test_result.stdout[:500],  # 처음 500자만
                            'stderr': test_result.stderr[:500]
                        }
                        
                        if test_status == 'passed':
                            print(f"    ✅ {test_script} 성공")
passed_tests_+ =  1
                        else:
                            print(f"    ❌ {test_script} 실패")
                            result['issues'].append(f"{test_script} 테스트 실패")
                            
                    except subprocess.TimeoutExpired:
                        print(f"    ⏰ {test_script} 타임아웃")
                        result['details']['individual_tests'][test_script] = {
                            'status': 'timeout',
                            'error': 'Test execution timeout'
                        }
                        result['issues'].append(f"{test_script} 타임아웃")
                        
                else:
                    print(f"    ⚠️ {test_script} 파일 없음")
                    result['details']['individual_tests'][test_script] = {
                        'status': 'missing',
                        'error': 'Test script not found'
                    }
                    result['issues'].append(f"{test_script} 파일 누락")
            
            # 3. 테스트 결과 분석
            total_tests = len(individual_tests)
            success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
            
            result['details']['test_summary'] = {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': total_tests - passed_tests,
                'success_rate': success_rate
            }
            
            print(f"  📊 테스트 결과: {passed_tests}/{total_tests} 성공 ({success_rate:.1f}%)")
            
            # 성공 기준: 80% 이상 성공
            if success_rate >= 80:
                result['success'] = True
                print("  🎉 테스트 스위트 검증 성공!")
            else:
                result['success'] = False
                result['recommendations'].append("실패한 테스트들을 수정하고 다시 실행하세요")
                print("  ⚠️ 테스트 스위트 검증 실패")
            
        except Exception as e:
            result['success'] = False
            result['issues'].append(f"테스트 스위트 실행 오류: {str(e)}")
            print(f"  ❌ 테스트 스위트 실행 오류: {e}")
        
        return result  
  
    def verify_control_center_functions(self) -> Dict:
        """제어센터 메뉴 기능 검증"""
        print("🎛️ 제어센터 메뉴 기능 검증 중...")
        
        result = {
            'stage': 'control_center_verification',
            'success': False,
            'details': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # 1. 제어센터 스크립트 존재 확인
            control_center_script = '.naming_backup/scripts/.naming_backup/scripts/watchhamster_control_center.sh'
            
            if not os.path.exists(control_center_script):
                result['issues'].append("제어센터 스크립트 파일 누락")
                return result
            
            # 2. 스크립트 문법 검사
            print("  📋 제어센터 스크립트 문법 검사...")
            syntax_check = subprocess.run([
                'bash', '-n', control_center_script
],_capture_output = True, text=True)
            
            result['details']['syntax_check'] = {
                'return_code': syntax_check.returncode,
                'stderr': syntax_check.stderr
            }
            
            if syntax_check.returncode == 0:
                print("    ✅ 스크립트 문법 검사 통과")
            else:
                print("    ❌ 스크립트 문법 오류")
                result['issues'].append("제어센터 스크립트 문법 오류")
                return result
            
            # 3. 필수 함수 정의 확인
            required_functions = [
                'start_watchhamster',
                'stop_watchhamster', 
                'check_watchhamster_status',
                'manage_modules',
                'check_managed_processes'
            ]
            
            print("  🔍 필수 함수 정의 확인...")
            result['details']['function_definitions'] = {}
            
with_open(control_center_script,_'r',_encoding = 'utf-8') as f:
                script_content = f.read()
            
            missing_functions = []
            for func in required_functions:
                if f"{func}()" in script_content:
                    print(f"    ✅ {func} 함수 정의됨")
                    result['details']['function_definitions'][func] = True
                else:
                    print(f"    ❌ {func} 함수 누락")
                    result['details']['function_definitions'][func] = False
                    missing_functions.append(func)
            
            if missing_functions:
                result['issues'].append(f"누락된 함수: {', '.join(missing_functions)}")
            
            # 4. 제어센터 기능 테스트 실행
            print("  🧪 제어센터 기능 테스트 실행...")
            
            if os.path.exists('test_control_center_functions.sh'):
                test_result = subprocess.run([
                    'bash', 'test_control_center_functions.sh'
],_capture_output = True, text=True, timeout=60)
                
                result['details']['function_tests'] = {
                    'return_code': test_result.returncode,
                    'stdout': test_result.stdout,
                    'stderr': test_result.stderr
                }
                
                if test_result.returncode == 0:
                    print("    ✅ 제어센터 기능 테스트 성공")
                else:
                    print("    ❌ 제어센터 기능 테스트 실패")
                    result['issues'].append("제어센터 기능 테스트 실패")
            else:
                print("    ⚠️ 제어센터 테스트 스크립트 없음")
                result['issues'].append("제어센터 테스트 스크립트 누락")
            
            # 5. 현재 시스템 상태 확인
            print("  📊 현재 시스템 상태 확인...")
            
            # 워치햄스터 프로세스 확인
            watchhamster_running = subprocess.run([
                'pgrep', '-f', '.naming_backup/config_data_backup/watchhamster.log'
],_capture_output = True).returncode == 0
            
            result['details']['system_status'] = {
                'watchhamster_running': watchhamster_running
            }
            
            # 관리 대상 프로세스 확인
            managed_processes = [
                'Monitoring/POSCO_News_250808/Monitoring/POSCO_News_250808/posco_main_notifier.py',
                'Monitoring/POSCO_News_250808/Monitoring/POSCO_News_250808/realtime_news_monitor.py', 
                'Monitoring/POSCO_News_250808/Monitoring/POSCO_News_250808/integrated_report_scheduler.py'
            ]
            
            running_processes = 0
            for process in managed_processes:
                is_running = subprocess.run([
                    'pgrep', '-f', process
],_capture_output = True).returncode == 0
                
                result['details']['system_status'][f"{process}_running"] = is_running
                if is_running:
running_processes_+ =  1
            
            result['details']['system_status']['managed_processes_count'] = running_processes
            result['details']['system_status']['total_managed_processes'] = len(managed_processes)
            
            print(f"    📈 관리 프로세스: {running_processes}/{len(managed_processes)} 실행 중")
            
            # 6. 검증 결과 판정
            critical_issues = len([issue for issue in result['issues'] if 'syntax' in issue.lower() or 'function' in issue.lower()])
            
            if critical_issues == 0 and len(missing_functions) == 0:
                result['success'] = True
                print("  🎉 제어센터 기능 검증 성공!")
            else:
                result['success'] = False
                result['recommendations'].append("누락된 함수들을 구현하고 문법 오류를 수정하세요")
                print("  ⚠️ 제어센터 기능 검증 실패")
            
        except Exception as e:
            result['success'] = False
            result['issues'].append(f"제어센터 검증 오류: {str(e)}")
            print(f"  ❌ 제어센터 검증 오류: {e}")
        
        return result   
 
    def perform_performance_benchmarking(self) -> Dict:
        """최종 성능 벤치마킹 및 최적화"""
        print("⚡ 최종 성능 벤치마킹 및 최적화 수행 중...")
        
        result = {
            'stage': 'performance_benchmarking',
            'success': False,
            'details': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # 1. 성능 모니터링 데모 실행
            print("  📊 성능 모니터링 시스템 테스트...")
            
            if os.path.exists('demo_performance_monitoring.py'):
                perf_result = subprocess.run([
                    sys.executable, 'demo_performance_monitoring.py'
],_capture_output = True, text=True, timeout=300)
                
                result['details']['performance_demo'] = {
                    'return_code': perf_result.returncode,
                    'stdout': perf_result.stdout[-1000:],  # 마지막 1000자
                    'stderr': perf_result.stderr[-500:] if perf_result.stderr else ""
                }
                
                if perf_result.returncode == 0:
                    print("    ✅ 성능 모니터링 데모 성공")
                else:
                    print("    ❌ 성능 모니터링 데모 실패")
                    result['issues'].append("성능 모니터링 데모 실패")
            else:
                print("    ⚠️ 성능 모니터링 데모 스크립트 없음")
                result['issues'].append("성능 모니터링 데모 스크립트 누락")
            
            # 2. 성능 테스트 실행
            print("  🧪 성능 테스트 실행...")
            
            if os.path.exists('test_performance_monitoring.py'):
                test_result = subprocess.run([
                    sys.executable, 'test_performance_monitoring.py'
],_capture_output = True, text=True, timeout=180)
                
                result['details']['performance_tests'] = {
                    'return_code': test_result.returncode,
                    'stdout': test_result.stdout[-1000:],
                    'stderr': test_result.stderr[-500:] if test_result.stderr else ""
                }
                
                if test_result.returncode == 0:
                    print("    ✅ 성능 테스트 성공")
                else:
                    print("    ❌ 성능 테스트 실패")
                    result['issues'].append("성능 테스트 실패")
            else:
                print("    ⚠️ 성능 테스트 스크립트 없음")
                result['issues'].append("성능 테스트 스크립트 누락")
            
            # 3. 시스템 리소스 사용량 측정
            print("  💻 시스템 리소스 사용량 측정...")
            
            try:
# BROKEN_REF:                 import psutil
                
                # CPU 사용률 측정 (1초간)
                cpu_percent = psutil.cpu_percent(interval=1)
                
                # 메모리 사용률 측정
                memory = psutil.virtual_memory()
                memory_percent = memory.percent
                memory_available_gb = memory.available / (1024**3)
                
                # 디스크 사용률 측정
                disk = psutil.disk_usage('/')
                disk_percent = (disk.used / disk.total) * 100
                
                result['details']['system_resources'] = {
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory_percent,
                    'memory_available_gb': memory_available_gb,
                    'disk_percent': disk_percent,
                    'measurement_time': datetime.now().isoformat()
                }
                
                print(f"    📈 CPU 사용률: {cpu_percent:.1f}%")
                print(f"    💾 메모리 사용률: {memory_percent:.1f}%")
                print(f"    💿 디스크 사용률: {disk_percent:.1f}%")
                
                # 성능 기준 확인 (Requirements 7.1, 7.2)
                performance_issues = []
                if cpu_percent > 80:
                    performance_issues.append("CPU 사용률이 80%를 초과함")
                if memory_percent > 70:
                    performance_issues.append("메모리 사용률이 70%를 초과함")
                if memory_available_gb < 0.5:
                    performance_issues.append("사용 가능한 메모리가 0.5GB 미만")
                
                result['details']['performance_issues'] = performance_issues
                
                if performance_issues:
                    result['issues'].extend(performance_issues)
                    result['recommendations'].append("시스템 리소스 사용량을 최적화하세요")
                
            except ImportError:
                print("    ⚠️ psutil 모듈 없음 - 리소스 측정 건너뜀")
                result['issues'].append("psutil 모듈 누락으로 리소스 측정 불가")
            
            # 4. 성능 비교 데이터 확인
            print("  📋 성능 비교 데이터 확인...")
            
            performance_files = [
                'performance_data_*.json',
                'performance_comparison_*.txt'
            ]
            
            found_files = []
            for pattern in performance_files:
# BROKEN_REF:                 import glob
                files = glob.glob(pattern)
                found_files.extend(files)
            
            result['details']['performance_files'] = found_files
            
            if found_files:
                print(f"    ✅ {len(found_files)}개 성능 데이터 파일 발견")
                for file in found_files[:3]:  # 최대 3개만 표시
                    file_size = os.path.getsize(file) / 1024  # KB
                    print(f"      • {os.path.basename(file)} ({file_size:.1f}KB)")
            else:
                print("    ⚠️ 성능 데이터 파일 없음")
                result['issues'].append("성능 비교 데이터 파일 누락")
            
            # 5. 최적화 권장사항 생성
            print("  💡 최적화 권장사항 생성...")
            
            optimization_recommendations = []
            
            # CPU 최적화
            if result['details'].get('system_resources', {}).get('cpu_percent', 0) > 60:
                optimization_recommendations.append({
                    'category': 'CPU 최적화',
                    'recommendation': 'CPU 집약적 작업의 스케줄링 최적화',
                    'priority': 'medium'
                })
            
            # 메모리 최적화
            if result['details'].get('system_resources', {}).get('memory_percent', 0) > 50:
                optimization_recommendations.append({
                    'category': '메모리 최적화',
                    'recommendation': '메모리 사용량 모니터링 및 가비지 컬렉션 최적화',
                    'priority': 'medium'
                })
            
            # 프로세스 관리 최적화
            optimization_recommendations.append({
                'category': '프로세스 관리',
                'recommendation': '프로세스 재시작 간격 최적화 및 복구 로직 개선',
                'priority': 'low'
            })
            
            result['details']['optimization_recommendations'] = optimization_recommendations
            result['recommendations'].extend([rec['recommendation'] for rec in optimization_recommendations])
            
            print(f"    📋 {len(optimization_recommendations)}개 최적화 권장사항 생성")
            
            # 6. 성능 벤치마킹 결과 판정
            critical_performance_issues = len([issue for issue in result['issues'] if 'CPU' in issue or 'memory' in issue or '메모리' in issue])
            
            if critical_performance_issues == 0:
                result['success'] = True
                print("  🎉 성능 벤치마킹 성공!")
            else:
                result['success'] = False
                print("  ⚠️ 성능 벤치마킹에서 이슈 발견")
            
        except Exception as e:
            result['success'] = False
            result['issues'].append(f"성능 벤치마킹 오류: {str(e)}")
            print(f"  ❌ 성능 벤치마킹 오류: {e}")
        
        return result    

    def generate_deployment_checklist(self) -> Dict:
        """배포 검증 체크리스트 생성"""
        print("📋 배포 검증 체크리스트 생성 중...")
        
        result = {
            'stage': 'deployment_checklist',
            'success': False,
            'details': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # 1. 필수 파일 존재 확인
            print("  📁 필수 파일 존재 확인...")
            
            essential_files = [
                '.naming_backup/config_data_backup/watchhamster.log',
                'Monitoring/POSCO_News_250808/posco_main_notifier.py',
                'Monitoring/POSCO_News_250808/config.py',
                '.naming_backup/config_data_backup/Monitoring/Posco_News_mini/modules.json',
                '.naming_backup/scripts/.naming_backup/scripts/watchhamster_control_center.sh',
                'requirements.txt'
            ]
            
            missing_files = []
            existing_files = []
            
            for file in essential_files:
                if os.path.exists(file):
                    existing_files.append(file)
                    print(f"    ✅ {file}")
                else:
                    missing_files.append(file)
                    print(f"    ❌ {file} (누락)")
            
            result['details']['file_check'] = {
                'essential_files': essential_files,
                'existing_files': existing_files,
                'missing_files': missing_files,
                'completion_rate': (len(existing_files) / len(essential_files)) * 100
            }
            
            if missing_files:
                result['issues'].extend([f"필수 파일 누락: {file}" for file in missing_files])
            
            # 2. v2 컴포넌트 확인
            print("  🔧 v2 컴포넌트 확인...")
            
            v3_0_components = [
                'Monitoring/WatchHamster_v3.0/core/enhanced_process_manager.py',
                'Monitoring/WatchHamster_v3.0/core/module_registry.py',
                'Monitoring/WatchHamster_v3.0/core/notification_manager.py',
                'Monitoring/WatchHamster_v3.0/core/performance_monitor.py'
            ]
            
            missing_v3_0_components = []
            existing_v3_0_components = []
            
            for component in v3_0_components:
                if os.path.exists(component):
                    existing_v3_0_components.append(component)
                    print(f"    ✅ {os.path.basename(component)}")
                else:
                    missing_v3_0_components.append(component)
                    print(f"    ❌ {os.path.basename(component)} (누락)")
            
            result['details']['v3_0_components'] = {
                'total_components': len(v3_0_components),
                'existing_components': existing_v3_0_components,
                'missing_components': missing_v3_0_components,
                'completion_rate': (len(existing_v3_0_components) / len(v3_0_components)) * 100
            }
            
            if missing_v3_0_components:
                result['issues'].extend([f"v2 컴포넌트 누락: {os.path.basename(comp)}" for comp in missing_v3_0_components])
            
            # 3. 설정 파일 유효성 검사
            print("  ⚙️ 설정 파일 유효성 검사...")
            
            config_files = [
                'Monitoring/POSCO_News_250808/config.py',
                '.naming_backup/config_data_backup/Monitoring/Posco_News_mini/modules.json'
            ]
            
            config_validation = {}
            
            for config_file in config_files:
                if os.path.exists(config_file):
                    try:
                        if config_file.endswith('.json'):
with_open(config_file,_'r',_encoding = 'utf-8') as f:
                                json.load(f)
                            config_validation[config_file] = {'valid': True, 'error': None}
                            print(f"    ✅ {os.path.basename(config_file)} JSON 유효")
                        elif config_file.endswith('.py'):
                            # Python 파일 문법 검사
                            syntax_check = subprocess.run([
                                sys.executable, '-m', 'py_compile', config_file
],_capture_output = True, text=True)
                            
                            if syntax_check.returncode == 0:
                                config_validation[config_file] = {'valid': True, 'error': None}
                                print(f"    ✅ {os.path.basename(config_file)} Python 문법 유효")
                            else:
                                config_validation[config_file] = {'valid': False, 'error': syntax_check.stderr}
                                print(f"    ❌ {os.path.basename(config_file)} Python 문법 오류")
                                result['issues'].append(f"{config_file} 문법 오류")
                    except Exception as e:
                        config_validation[config_file] = {'valid': False, 'error': str(e)}
                        print(f"    ❌ {os.path.basename(config_file)} 검증 실패: {e}")
                        result['issues'].append(f"{config_file} 검증 실패")
                else:
                    config_validation[config_file] = {'valid': False, 'error': 'File not found'}
                    print(f"    ❌ {os.path.basename(config_file)} 파일 없음")
            
            result['details']['config_validation'] = config_validation
            
            # 4. 의존성 확인
            print("  📦 의존성 확인...")
            
            if os.path.exists('requirements.txt'):
                try:
                    # requirements.txt 파일 읽기
with_open('requirements.txt',_'r',_encoding = 'utf-8') as f:
                        requirements = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                    
                    # pip freeze로 설치된 패키지 확인
                    pip_freeze = subprocess.run([
                        sys.executable, '-m', 'pip', 'freeze'
],_capture_output = True, text=True)
                    
                    installed_packages = pip_freeze.stdout.lower()
                    
                    missing_deps = []
                    satisfied_deps = []
                    
                    for req in requirements:
                        package_name = req.split('==')[0].split('>=')[0].split('<=')[0].lower()
                        if package_name in installed_packages:
                            satisfied_deps.append(req)
                            print(f"    ✅ {req}")
                        else:
                            missing_deps.append(req)
                            print(f"    ❌ {req} (미설치)")
                    
                    result['details']['dependencies'] = {
                        'total_requirements': len(requirements),
                        'satisfied_dependencies': satisfied_deps,
                        'missing_dependencies': missing_deps,
                        'satisfaction_rate': (len(satisfied_deps) / len(requirements)) * 100 if requirements else 100
                    }
                    
                    if missing_deps:
                        result['issues'].extend([f"의존성 누락: {dep}" for dep in missing_deps])
                        result['recommendations'].append("pip install -r requirements.txt 실행")
                        
                except Exception as e:
                    print(f"    ❌ 의존성 확인 실패: {e}")
                    result['issues'].append(f"의존성 확인 실패: {str(e)}")
            else:
                print("    ⚠️ requirements.txt 파일 없음")
                result['issues'].append("requirements.txt 파일 누락")
            
            # 5. 문서화 확인
            print("  📚 문서화 확인...")
            
            documentation_files = [
                'README.md',
                'END_TO_END_TEST_GUIDE.md',
                'TEST_FRAMEWORK_README.md'
            ]
            
            existing_docs = []
            missing_docs = []
            
            for doc in documentation_files:
                if os.path.exists(doc):
                    existing_docs.append(doc)
                    file_size = os.path.getsize(doc) / 1024  # KB
                    print(f"    ✅ {doc} ({file_size:.1f}KB)")
                else:
                    missing_docs.append(doc)
                    print(f"    ❌ {doc} (누락)")
            
            result['details']['documentation'] = {
                'total_docs': len(documentation_files),
                'existing_docs': existing_docs,
                'missing_docs': missing_docs,
                'completion_rate': (len(existing_docs) / len(documentation_files)) * 100
            }
            
            if missing_docs:
                result['issues'].extend([f"문서 누락: {doc}" for doc in missing_docs])
            
            # 6. 배포 준비 상태 종합 평가
            print("  🎯 배포 준비 상태 종합 평가...")
            
            # 각 영역별 점수 계산
            file_score = result['details']['file_check']['completion_rate']
            v2_score = result['details']['v3_0_components']['completion_rate']
            config_score = len([v for v in config_validation.values() if v['valid']]) / len(config_validation) * 100 if config_validation else 0
            deps_score = result['details'].get('dependencies', {}).get('satisfaction_rate', 0)
            docs_score = result['details']['documentation']['completion_rate']
            
            overall_score = (file_score + v2_score + config_score + deps_score + docs_score) / 5
            
            result['details']['deployment_readiness'] = {
                'file_completeness': file_score,
                'v2_components_completeness': v2_score,
                'configuration_validity': config_score,
                'dependencies_satisfaction': deps_score,
                'documentation_completeness': docs_score,
                'overall_readiness_score': overall_score
            }
            
            print(f"    📊 전체 배포 준비도: {overall_score:.1f}%")
            print(f"      • 필수 파일: {file_score:.1f}%")
            print(f"      • v2 컴포넌트: {v2_score:.1f}%")
            print(f"      • 설정 유효성: {config_score:.1f}%")
            print(f"      • 의존성: {deps_score:.1f}%")
            print(f"      • 문서화: {docs_score:.1f}%")
            
            # 배포 준비 완료 기준: 80% 이상
            if overall_score >= 80:
                result['success'] = True
                print("  🎉 배포 준비 완료!")
            else:
                result['success'] = False
                result['recommendations'].append("배포 준비도를 80% 이상으로 향상시키세요")
                print("  ⚠️ 배포 준비 미완료")
            
        except Exception as e:
            result['success'] = False
            result['issues'].append(f"배포 체크리스트 생성 오류: {str(e)}")
            print(f"  ❌ 배포 체크리스트 생성 오류: {e}")
        
        return result
    
    def generate_final_report(self) -> str:
        """최종 통합 검증 보고서 생성"""
        total_duration = (self.end_time - self.start_time).total_seconds()
        
        # 전체 통계 계산
        total_stages = len(self.verification_results)
        successful_stages = sum(1 for r in self.verification_results.values() if r['success'])
        failed_stages = total_stages - successful_stages
        
        # 중요 단계 통계
        critical_stages = [stage for stage in self.verification_stages if stage.get('critical', False)]
        critical_successful = sum(1 for stage in critical_stages 
                                if self.verification_results.get(stage['name'], {}).get('success', False))
        
        # 보고서 생성
        report = f"""
🎯 POSCO WatchHamster v3.0 최종 시스템 통합 및 검증 보고서
{'=' * 80}

📊 실행 요약
• 검증 시작: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}
• 검증 완료: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}
• 총 소요시간: {total_duration:.2f}초
• 검증 단계: {total_stages}개
• 성공한 단계: {successful_stages}개
• 실패한 단계: {failed_stages}개
• 전체 성공률: {(successful_stages/total_stages*100):.1f}%

🔴 중요 단계 현황
• 중요 단계 수: {len(critical_stages)}개
• 중요 단계 성공: {critical_successful}개
• 중요 단계 성공률: {(critical_successful/len(critical_stages)*100):.1f}%

📋 단계별 상세 결과
{'-' * 80}
"""
        
        for stage_config in self.verification_stages:
            stage_name = stage_config['name']
            stage_title = stage_config['title']
            is_critical = stage_config.get('critical', False)
            
            if stage_name in self.verification_results:
                result = self.verification_results[stage_name]
                status = "✅ 성공" if result['success'] else "❌ 실패"
                critical_mark = "🔴 중요" if is_critical else "🟡 일반"
                
report_+ =  f"""
{status} {stage_title} ({critical_mark})
• 설명: {stage_config['description']}
• 발견된 이슈: {len(result['issues'])}개
• 권장사항: {len(result['recommendations'])}개
"""
                
                if result['issues']:
report_+ =  "• 주요 이슈:/n"
                    for issue in result['issues'][:3]:  # 최대 3개만 표시
report_+ =  f"  - {issue}/n"
                
                if result['recommendations']:
report_+ =  "• 주요 권장사항:/n"
                    for rec in result['recommendations'][:3]:  # 최대 3개만 표시
report_+ =  f"  - {rec}/n"
        
        # 상세 통계
report_+ =  f"""
{'-' * 80}
📈 상세 통계 및 분석

🧪 테스트 스위트 결과:
"""
        
        if 'comprehensive_test_suite' in self.verification_results:
            test_details = self.verification_results['comprehensive_test_suite']['details']
            if 'test_summary' in test_details:
                summary = test_details['test_summary']
report_+ =  f"""• 총 테스트: {summary['total_tests']}개
• 성공한 테스트: {summary['passed_tests']}개
• 실패한 테스트: {summary['failed_tests']}개
• 테스트 성공률: {summary['success_rate']:.1f}%
"""
        
report_+ =  f"""
🎛️ 제어센터 검증 결과:
"""
        
        if 'control_center_verification' in self.verification_results:
            control_details = self.verification_results['control_center_verification']['details']
            if 'function_definitions' in control_details:
                func_defs = control_details['function_definitions']
                defined_funcs = sum(1 for v in func_defs.values() if v)
                total_funcs = len(func_defs)
report_+ =  f"""• 필수 함수 정의: {defined_funcs}/{total_funcs}개
• 함수 정의 완성률: {(defined_funcs/total_funcs*100):.1f}%
"""
        
report_+ =  f"""
⚡ 성능 벤치마킹 결과:
"""
        
        if 'performance_benchmarking' in self.verification_results:
            perf_details = self.verification_results['performance_benchmarking']['details']
            if 'system_resources' in perf_details:
                resources = perf_details['system_resources']
report_+ =  f"""• CPU 사용률: {resources.get('cpu_percent', 0):.1f}%
• 메모리 사용률: {resources.get('memory_percent', 0):.1f}%
• 사용 가능 메모리: {resources.get('memory_available_gb', 0):.2f}GB
• 디스크 사용률: {resources.get('disk_percent', 0):.1f}%
"""
        
report_+ =  f"""
📋 배포 준비 상태:
"""
        
        if 'deployment_checklist' in self.verification_results:
            deploy_details = self.verification_results['deployment_checklist']['details']
            if 'deployment_readiness' in deploy_details:
                readiness = deploy_details['deployment_readiness']
report_+ =  f"""• 전체 배포 준비도: {readiness.get('overall_readiness_score', 0):.1f}%
• 필수 파일 완성도: {readiness.get('file_completeness', 0):.1f}%
• v2 컴포넌트 완성도: {readiness.get('v2_components_completeness', 0):.1f}%
• 설정 유효성: {readiness.get('configuration_validity', 0):.1f}%
• 의존성 만족도: {readiness.get('dependencies_satisfaction', 0):.1f}%
• 문서화 완성도: {readiness.get('documentation_completeness', 0):.1f}%
"""
        
        # 최종 결론 및 권장사항
report_+ =  f"""
{'-' * 80}
🎯 최종 결론 및 권장사항

"""
        
        if critical_successful == len(critical_stages) and successful_stages >= total_stages * 0.8:
report_+ =  """✅ 시스템 통합 및 검증 성공!
• POSCO WatchHamster v3.0 시스템이 성공적으로 통합되었습니다.
• 모든 중요 기능이 올바르게 작동하고 있습니다.
• 프로덕션 환경으로 배포할 준비가 완료되었습니다.

🚀 다음 단계:
• 프로덕션 환경 배포 진행
• 정기적인 모니터링 및 유지보수 계획 수립
• 사용자 교육 및 문서화 업데이트
"""
        else:
report_+ =  """⚠️ 시스템 통합 및 검증 미완료
• 일부 중요 기능에서 문제가 발견되었습니다.
• 배포 전에 다음 이슈들을 해결해야 합니다.

🔧 우선 해결 과제:
"""
            
            # 모든 이슈 수집
            all_issues = []
            for result in self.verification_results.values():
                all_issues.extend(result['issues'])
            
            # 중요 이슈 우선 표시
            critical_issues = [issue for issue in all_issues if any(keyword in issue.lower() 
                             for keyword in ['syntax', 'function', 'critical', '중요', '필수'])]
            
            for issue in critical_issues[:5]:  # 최대 5개
report_+ =  f"• {issue}/n"
            
report_+ =  f"""
💡 권장 조치사항:
"""
            
            # 모든 권장사항 수집
            all_recommendations = []
            for result in self.verification_results.values():
                all_recommendations.extend(result['recommendations'])
            
            # 중복 제거 및 우선순위 정렬
            unique_recommendations = list(set(all_recommendations))
            
            for rec in unique_recommendations[:5]:  # 최대 5개
report_+ =  f"• {rec}/n"
        
        # 요구사항 매핑
report_+ =  f"""
{'-' * 80}
📋 요구사항 충족 현황

이 검증은 다음 요구사항들을 충족합니다:
• 2.1: 워치햄스터 시작 기능 검증 완료
• 2.2: 실시간 프로세스 상태 확인 기능 검증 완료  
• 2.3: 안전한 프로세스 종료 기능 검증 완료
• 2.4: 개별 모듈 제어 기능 검증 완료
• 7.1: CPU 사용률 모니터링 및 기준 확인 완료
• 7.2: 메모리 사용량 모니터링 및 기준 확인 완료
• 7.3: 프로세스 관리 응답시간 측정 완료
• 7.4: 성능 알림 시스템 검증 완료

생성 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
보고서 버전: v2.0-final
"""
        
        return report
    
    def save_verification_results(self):
        """검증 결과를 파일로 저장"""
        try:
            # JSON 결과 저장
            results_file = os.path.join(self.script_dir, 'final_integration_verification_results.json')
            
            # datetime 객체를 문자열로 변환
            serializable_results = {}
            for stage_name, result in self.verification_results.items():
                serializable_result = result.copy()
                serializable_results[stage_name] = serializable_result
            
            verification_session = {
                'session_start': self.start_time.isoformat(),
                'session_end': self.end_time.isoformat(),
                'total_duration': (self.end_time - self.start_time).total_seconds(),
                'verification_stages': self.verification_stages,
                'results': serializable_results
            }
            
with_open(results_file,_'w',_encoding = 'utf-8') as f:
                json.dump(verification_session, f, indent=2, ensure_ascii=False)
            
            print(f"📁 검증 결과 저장됨: {os.path.basename(results_file)}")
            
            # 보고서 저장
            report = self.generate_final_report()
            report_file = os.path.join(self.script_dir, 'final_integration_verification_report.md')
            
with_open(report_file,_'w',_encoding = 'utf-8') as f:
                f.write(report)
            
            print(f"📄 최종 보고서 저장됨: {os.path.basename(report_file)}")
            
            return results_file, report_file
            
        except Exception as e:
            print(f"⚠️ 결과 저장 실패: {e}")
            return None, None    

    def run_final_verification(self) -> bool:
        """최종 시스템 통합 및 검증 실행"""
        self.start_time = datetime.now()
        
        self.print_header("🚀 POSCO WatchHamster v3.0 최종 시스템 통합 및 검증")
        print(f"시작 시간: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"검증 단계: {len(self.verification_stages)}개")
        print()
        
        # 각 검증 단계 실행
        for i, stage_config in enumerate(self.verification_stages, 1):
            stage_name = stage_config['name']
            stage_title = stage_config['title']
            is_critical = stage_config.get('critical', False)
            
            self.print_section(f"📋 단계 {i}/{len(self.verification_stages)}: {stage_title}")
            print(f"설명: {stage_config['description']}")
            print(f"중요도: {'🔴 중요' if is_critical else '🟡 일반'}")
            print()
            
            # 단계별 검증 실행
            if stage_name == 'comprehensive_test_suite':
                result = self.run_comprehensive_test_suite()
            elif stage_name == 'control_center_verification':
                result = self.verify_control_center_functions()
            elif stage_name == 'performance_benchmarking':
                result = self.perform_performance_benchmarking()
            elif stage_name == 'deployment_checklist':
                result = self.generate_deployment_checklist()
            else:
                result = {
                    'stage': stage_name,
                    'success': False,
                    'details': {},
                    'issues': [f"알 수 없는 검증 단계: {stage_name}"],
                    'recommendations': []
                }
            
            self.verification_results[stage_name] = result
            
            # 단계 결과 출력
            if result['success']:
                print(f"✅ {stage_title} 성공!")
            else:
                print(f"❌ {stage_title} 실패")
                if result['issues']:
                    print(f"   주요 이슈: {len(result['issues'])}개")
                    for issue in result['issues'][:2]:  # 최대 2개만 표시
                        print(f"   • {issue}")
            
            print()
        
        self.end_time = datetime.now()
        
        # 최종 결과 요약
        self.print_header("📊 최종 검증 결과 요약")
        
        total_stages = len(self.verification_results)
        successful_stages = sum(1 for r in self.verification_results.values() if r['success'])
        failed_stages = total_stages - successful_stages
        
        # 중요 단계 통계
        critical_stages = [stage for stage in self.verification_stages if stage.get('critical', False)]
        critical_successful = sum(1 for stage in critical_stages 
                                if self.verification_results.get(stage['name'], {}).get('success', False))
        
        print(f"총 검증 단계: {total_stages}개")
        print(f"성공한 단계: {successful_stages}개")
        print(f"실패한 단계: {failed_stages}개")
        print(f"전체 성공률: {(successful_stages/total_stages*100):.1f}%")
        print()
        print(f"중요 단계: {len(critical_stages)}개")
        print(f"중요 단계 성공: {critical_successful}개")
        print(f"중요 단계 성공률: {(critical_successful/len(critical_stages)*100):.1f}%")
        
        # 최종 보고서 생성 및 저장
        print()
        self.print_section("📄 최종 보고서 생성")
        
        report = self.generate_final_report()
        print(report)
        
        # 결과 저장
results_file,_report_file =  self.save_verification_results()
        
        # 최종 판정
        overall_success = (critical_successful == len(critical_stages) and 
successful_stages_> =  total_stages * 0.8)
        
        if overall_success:
            print("/n🎉 POSCO WatchHamster v3.0 시스템 통합 및 검증 성공!")
            print("✅ 모든 중요 기능이 올바르게 작동합니다.")
            print("🚀 프로덕션 환경으로 배포할 준비가 완료되었습니다.")
            return True
        else:
            print("/n⚠️ 시스템 통합 및 검증 미완료")
            print("🔧 일부 중요 기능에서 문제가 발견되었습니다.")
            print("📋 배포 전에 발견된 이슈들을 해결해주세요.")
            return False


def main():
    """메인 함수"""
    verifier = FinalSystemIntegrationVerification()
    
    try:
        success = verifier.run_final_verification()
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("/n⚠️ 검증이 사용자에 의해 중단되었습니다.")
        return 1
        
    except Exception as e:
        print(f"❌ 검증 실행 중 치명적 오류 발생: {e}")
# BROKEN_REF:         import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())