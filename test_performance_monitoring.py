#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Performance Monitoring Integration Test

v2 ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- v1/v2 ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
- ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„± í…ŒìŠ¤íŠ¸
- ì‘ë‹µ ì‹œê°„ ì¸¡ì • í…ŒìŠ¤íŠ¸

Requirements: 7.1, 7.2, 7.3, 7.4
"""

import unittest
import sys
import os
import time
import json
import tempfile
import shutil
from datetime import datetime, timedelta
from unittest.mock import Mock, patch, MagicMock

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(current_dir, 'Monitoring', 'Posco_News_mini_v2'))

try:
    from core.performance_monitor import (
        PerformanceMonitor, PerformanceComparator, 
        SystemPerformanceSnapshot, ProcessPerformanceData,
        PerformanceMetric, MetricType, PerformanceLevel
    )
    from core.performance_optimizer import (
        PerformanceOptimizer, OptimizationRecommendation,
        PerformanceIssue, OptimizationCategory, OptimizationPriority
    )
except ImportError as e:
    print(f"âŒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    sys.exit(1)

class TestPerformanceMonitoring(unittest.TestCase):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.test_dir = tempfile.mkdtemp()
        self.performance_monitor = PerformanceMonitor(self.test_dir, monitoring_interval=1)
        self.performance_optimizer = PerformanceOptimizer(self.test_dir)
        self.performance_comparator = PerformanceComparator(self.test_dir)
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        if hasattr(self, 'performance_monitor'):
            self.performance_monitor.stop_monitoring()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
    
    def test_performance_monitor_initialization(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”§ ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
        
        # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
        self.assertIsNotNone(self.performance_monitor)
        self.assertEqual(self.performance_monitor.script_dir, self.test_dir)
        self.assertFalse(self.performance_monitor.is_monitoring)
        self.assertEqual(len(self.performance_monitor.metrics_history), 0)
        
        print("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™” ì„±ê³µ")
    
    def test_system_snapshot_collection(self):
        """ì‹œìŠ¤í…œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š ì‹œìŠ¤í…œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸...")
        
        # ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘
        snapshot = self.performance_monitor._collect_system_snapshot()
        
        # ìŠ¤ëƒ…ìƒ· ë°ì´í„° ê²€ì¦
        self.assertIsInstance(snapshot, SystemPerformanceSnapshot)
        self.assertIsInstance(snapshot.timestamp, datetime)
        self.assertGreaterEqual(snapshot.cpu_percent, 0)
        self.assertGreaterEqual(snapshot.memory_percent, 0)
        self.assertGreaterEqual(snapshot.memory_available_mb, 0)
        self.assertGreaterEqual(snapshot.process_count, 0)
        self.assertIsInstance(snapshot.active_processes, list)
        
        print(f"âœ… ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ì„±ê³µ: CPU {snapshot.cpu_percent:.1f}%, ë©”ëª¨ë¦¬ {snapshot.memory_percent:.1f}%")
    
    def test_performance_monitoring_start_stop(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘/ì¤‘ì§€ í…ŒìŠ¤íŠ¸"""
        print("\nğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘/ì¤‘ì§€ í…ŒìŠ¤íŠ¸...")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_monitoring()
        self.assertTrue(self.performance_monitor.is_monitoring)
        self.assertIsNotNone(self.performance_monitor.monitoring_thread)
        
        # ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
        time.sleep(3)
        
        # ë°ì´í„° ìˆ˜ì§‘ í™•ì¸
        self.assertGreater(len(self.performance_monitor.metrics_history), 0)
        self.assertGreater(self.performance_monitor.total_measurements, 0)
        
        # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.performance_monitor.stop_monitoring()
        self.assertFalse(self.performance_monitor.is_monitoring)
        
        print("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘/ì¤‘ì§€ ì„±ê³µ")
    
    def test_response_time_measurement(self):
        """ì‘ë‹µ ì‹œê°„ ì¸¡ì • í…ŒìŠ¤íŠ¸"""
        print("\nâ±ï¸ ì‘ë‹µ ì‹œê°„ ì¸¡ì • í…ŒìŠ¤íŠ¸...")
        
        # ì‘ë‹µ ì‹œê°„ ì¸¡ì • (ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©)
        with self.performance_monitor.measure_operation_time("test_operation"):
            time.sleep(0.1)  # 0.1ì´ˆ ëŒ€ê¸°
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë¡ í™•ì¸
        self.assertIn("test_operation", self.performance_monitor.response_times)
        response_times = list(self.performance_monitor.response_times["test_operation"])
        self.assertEqual(len(response_times), 1)
        self.assertGreaterEqual(response_times[0], 0.1)
        self.assertLess(response_times[0], 0.2)  # 0.2ì´ˆ ë¯¸ë§Œì´ì–´ì•¼ í•¨
        
        # ì§ì ‘ ì‘ë‹µ ì‹œê°„ ê¸°ë¡
        self.performance_monitor.record_response_time("manual_operation", 2.5)
        self.assertIn("manual_operation", self.performance_monitor.response_times)
        manual_times = list(self.performance_monitor.response_times["manual_operation"])
        self.assertEqual(len(manual_times), 1)
        self.assertEqual(manual_times[0], 2.5)
        
        print("âœ… ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì„±ê³µ")
    
    def test_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“‹ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ í…ŒìŠ¤íŠ¸...")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
        self.performance_monitor.start_monitoring()
        time.sleep(2)
        
        # ì‘ë‹µ ì‹œê°„ ë°ì´í„° ì¶”ê°€
        self.performance_monitor.record_response_time("test_op", 1.5)
        self.performance_monitor.record_response_time("test_op", 2.0)
        
        # ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ
        summary = self.performance_monitor.get_performance_summary()
        
        # ìš”ì•½ ë°ì´í„° ê²€ì¦
        self.assertNotIn('error', summary)
        self.assertIn('timestamp', summary)
        self.assertIn('uptime_seconds', summary)
        self.assertIn('monitoring_status', summary)
        self.assertIn('current', summary)
        self.assertIn('averages', summary)
        self.assertIn('response_times', summary)
        self.assertIn('performance_level', summary)
        
        # í˜„ì¬ ìƒíƒœ ë°ì´í„° ê²€ì¦
        current = summary['current']
        self.assertIn('cpu_percent', current)
        self.assertIn('memory_percent', current)
        self.assertIn('process_count', current)
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„ ê²€ì¦
        response_times = summary['response_times']
        self.assertIn('test_op', response_times)
        test_op_stats = response_times['test_op']
        self.assertIn('avg', test_op_stats)
        self.assertIn('min', test_op_stats)
        self.assertIn('max', test_op_stats)
        self.assertEqual(test_op_stats['count'], 2)
        
        self.performance_monitor.stop_monitoring()
        
        print("âœ… ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì„±ê³µ")
    
    def test_performance_comparison(self):
        """ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸...")
        
        # v1 ê¸°ì¤€ì„  ë°ì´í„° ìƒì„±
        v1_baseline = {
            'cpu_percent': 50.0,
            'memory_percent': 60.0,
            'process_count': 10,
            'response_time_avg': 3.0
        }
        
        # v2 ì„±ëŠ¥ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
        self.performance_monitor.start_monitoring()
        time.sleep(2)
        self.performance_monitor.record_response_time("test_operation", 2.0)
        
        # ì„±ëŠ¥ ë¹„êµ ìˆ˜í–‰
        comparison = self.performance_monitor.compare_with_baseline(v1_baseline)
        
        # ë¹„êµ ê²°ê³¼ ê²€ì¦
        self.assertIsNotNone(comparison)
        self.assertIsInstance(comparison.comparison_time, datetime)
        self.assertIsInstance(comparison.v1_metrics, dict)
        self.assertIsInstance(comparison.v2_metrics, dict)
        self.assertIsInstance(comparison.improvement_percentage, dict)
        self.assertIsInstance(comparison.recommendations, list)
        
        # ê°œì„ ìœ¨ ê³„ì‚° ê²€ì¦
        self.assertIn('cpu_percent', comparison.improvement_percentage)
        self.assertIn('memory_percent', comparison.improvement_percentage)
        
        self.performance_monitor.stop_monitoring()
        
        print("âœ… ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
    def test_performance_optimizer_initialization(self):
        """ì„±ëŠ¥ ìµœì í™”ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”§ ì„±ëŠ¥ ìµœì í™”ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
        
        # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
        self.assertIsNotNone(self.performance_optimizer)
        self.assertEqual(self.performance_optimizer.script_dir, self.test_dir)
        self.assertEqual(len(self.performance_optimizer.recommendations), 0)
        self.assertEqual(len(self.performance_optimizer.applied_optimizations), 0)
        
        # ì„ê³„ê°’ ì„¤ì • í™•ì¸
        self.assertIn('cpu_high', self.performance_optimizer.thresholds)
        self.assertIn('memory_critical', self.performance_optimizer.thresholds)
        
        print("âœ… ì„±ëŠ¥ ìµœì í™”ê¸° ì´ˆê¸°í™” ì„±ê³µ")
    
    def test_performance_issue_analysis(self):
        """ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
        
        # ê³ ë¶€í•˜ ìƒí™© ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
        high_load_data = {
            'current': {
                'cpu_percent': 85.0,  # ì„ê³„ ìˆ˜ì¤€
                'memory_percent': 80.0,  # ë†’ì€ ìˆ˜ì¤€
                'disk_usage_percent': 70.0,
                'process_count': 20
            },
            'response_times': {
                'slow_operation': {'avg': 8.0, 'min': 5.0, 'max': 12.0}
            }
        }
        
        # ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„
        issues = self.performance_optimizer.analyze_system_performance(high_load_data)
        
        # ì´ìŠˆ ê²€ì¦
        self.assertGreater(len(issues), 0)
        
        # CPU ì´ìŠˆ í™•ì¸
        cpu_issues = [issue for issue in issues if issue.issue_type.startswith('cpu')]
        self.assertGreater(len(cpu_issues), 0)
        
        # ë©”ëª¨ë¦¬ ì´ìŠˆ í™•ì¸
        memory_issues = [issue for issue in issues if issue.issue_type.startswith('memory')]
        self.assertGreater(len(memory_issues), 0)
        
        # ì‘ë‹µì‹œê°„ ì´ìŠˆ í™•ì¸
        response_issues = [issue for issue in issues if issue.issue_type.startswith('response_time')]
        self.assertGreater(len(response_issues), 0)
        
        print(f"âœ… ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„ ì„±ê³µ: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")
    
    def test_optimization_recommendations(self):
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ìš© ì„±ëŠ¥ ì´ìŠˆ ìƒì„±
        test_issues = [
            PerformanceIssue(
                issue_type="cpu_critical",
                severity="critical",
                description="CPU ì‚¬ìš©ë¥ ì´ ì„ê³„ ìˆ˜ì¤€ì…ë‹ˆë‹¤",
                affected_components=["system"],
                metrics={"cpu_percent": 90.0},
                detected_at=datetime.now(),
                recommendations=["cpu_optimization"]
            ),
            PerformanceIssue(
                issue_type="memory_high",
                severity="warning",
                description="ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤",
                affected_components=["system"],
                metrics={"memory_percent": 80.0},
                detected_at=datetime.now(),
                recommendations=["memory_cleanup"]
            )
        ]
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self.performance_optimizer.generate_optimization_recommendations(test_issues)
        
        # ê¶Œì¥ì‚¬í•­ ê²€ì¦
        self.assertGreater(len(recommendations), 0)
        
        for rec in recommendations:
            self.assertIsInstance(rec, OptimizationRecommendation)
            self.assertIsInstance(rec.category, OptimizationCategory)
            self.assertIsInstance(rec.priority, OptimizationPriority)
            self.assertIsInstance(rec.implementation_steps, list)
            self.assertGreater(len(rec.implementation_steps), 0)
        
        print(f"âœ… ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„± ì„±ê³µ: {len(recommendations)}ê°œ ê¶Œì¥ì‚¬í•­")
    
    def test_optimization_summary(self):
        """ìµœì í™” ìš”ì•½ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š ìµœì í™” ìš”ì•½ ì¡°íšŒ í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ìš© ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        test_recommendation = OptimizationRecommendation(
            id="test_rec_001",
            category=OptimizationCategory.CPU,
            priority=OptimizationPriority.HIGH,
            title="í…ŒìŠ¤íŠ¸ CPU ìµœì í™”",
            description="í…ŒìŠ¤íŠ¸ìš© CPU ìµœì í™” ê¶Œì¥ì‚¬í•­",
            impact_description="ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ",
            implementation_steps=["1ë‹¨ê³„", "2ë‹¨ê³„"],
            estimated_improvement="20% ê°œì„ ",
            risk_level="ë‚®ìŒ",
            created_at=datetime.now()
        )
        
        self.performance_optimizer.recommendations.append(test_recommendation)
        
        # ìµœì í™” ìš”ì•½ ì¡°íšŒ
        summary = self.performance_optimizer.get_optimization_summary()
        
        # ìš”ì•½ ë°ì´í„° ê²€ì¦
        self.assertNotIn('error', summary)
        self.assertIn('timestamp', summary)
        self.assertIn('total_recommendations', summary)
        self.assertIn('applied_optimizations', summary)
        self.assertIn('pending_optimizations', summary)
        self.assertIn('category_breakdown', summary)
        self.assertIn('priority_breakdown', summary)
        self.assertIn('recent_recommendations', summary)
        
        # ë°ì´í„° ê°’ ê²€ì¦
        self.assertEqual(summary['total_recommendations'], 1)
        self.assertEqual(summary['applied_optimizations'], 0)
        self.assertEqual(summary['pending_optimizations'], 1)
        
        print("âœ… ìµœì í™” ìš”ì•½ ì¡°íšŒ ì„±ê³µ")
    
    def test_performance_data_export(self):
        """ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ’¾ ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸...")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
        self.performance_monitor.start_monitoring()
        time.sleep(2)
        self.performance_monitor.record_response_time("export_test", 1.0)
        
        # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        export_file = self.performance_monitor.export_performance_data()
        
        # íŒŒì¼ ìƒì„± í™•ì¸
        self.assertTrue(os.path.exists(export_file))
        self.assertTrue(export_file.endswith('.json'))
        
        # ë‚´ë³´ë‚¸ ë°ì´í„° ê²€ì¦
        with open(export_file, 'r', encoding='utf-8') as f:
            export_data = json.load(f)
        
        self.assertIn('export_timestamp', export_data)
        self.assertIn('monitoring_period', export_data)
        self.assertIn('summary', export_data)
        self.assertIn('metrics_history', export_data)
        self.assertIn('alert_count', export_data)
        
        self.performance_monitor.stop_monitoring()
        
        print(f"âœ… ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì„±ê³µ: {export_file}")
    
    def test_v1_baseline_collection(self):
        """v1 ê¸°ì¤€ì„  ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š v1 ê¸°ì¤€ì„  ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸...")
        
        # psutil ëª¨í‚¹ìœ¼ë¡œ v1 í”„ë¡œì„¸ìŠ¤ ì‹œë®¬ë ˆì´ì…˜
        with patch('psutil.process_iter') as mock_process_iter:
            # ëª¨í‚¹ëœ í”„ë¡œì„¸ìŠ¤ ë°ì´í„°
            mock_processes = [
                Mock(info={
                    'pid': 1234,
                    'name': 'python3',
                    'cmdline': ['python3', 'monitor_watchhamster.py'],
                    'cpu_percent': 15.0,
                    'memory_percent': 25.0
                }),
                Mock(info={
                    'pid': 1235,
                    'name': 'python3',
                    'cmdline': ['python3', 'posco_main_notifier.py'],
                    'cpu_percent': 10.0,
                    'memory_percent': 20.0
                })
            ]
            
            mock_process_iter.return_value = mock_processes
            
            # CPUì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ëª¨í‚¹
            with patch('psutil.cpu_percent', return_value=45.0), \
                 patch('psutil.virtual_memory') as mock_memory:
                
                mock_memory.return_value.percent = 55.0
                
                # v1 ê¸°ì¤€ì„  ìˆ˜ì§‘
                baseline = self.performance_comparator.collect_v1_baseline()
                
                # ê¸°ì¤€ì„  ë°ì´í„° ê²€ì¦
                self.assertIn('timestamp', baseline)
                self.assertIn('system_type', baseline)
                self.assertEqual(baseline['system_type'], 'v1')
                self.assertIn('cpu_percent', baseline)
                self.assertIn('memory_percent', baseline)
                self.assertIn('process_count', baseline)
                self.assertIn('processes', baseline)
                
                # í”„ë¡œì„¸ìŠ¤ ë°ì´í„° ê²€ì¦
                self.assertEqual(baseline['process_count'], 2)
                self.assertEqual(len(baseline['processes']), 2)
        
        print("âœ… v1 ê¸°ì¤€ì„  ìˆ˜ì§‘ ì„±ê³µ")
    
    def test_comparison_report_generation(self):
        """ë¹„êµ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“‹ ë¹„êµ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        v1_data = {
            'cpu_percent': 50.0,
            'memory_percent': 60.0,
            'process_count': 10,
            'response_time_avg': 3.0
        }
        
        v2_data = {
            'cpu_percent': 40.0,
            'memory_percent': 50.0,
            'process_count': 8,
            'response_time_avg': 2.0
        }
        
        # ë¹„êµ ë³´ê³ ì„œ ìƒì„±
        report = self.performance_comparator.generate_comparison_report(v1_data, v2_data)
        
        # ë³´ê³ ì„œ ë‚´ìš© ê²€ì¦
        self.assertIsInstance(report, str)
        self.assertIn("ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ", report)
        self.assertIn("CPU ì‚¬ìš©ë¥ ", report)
        self.assertIn("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ", report)
        self.assertIn("ê¶Œì¥ì‚¬í•­", report)
        self.assertIn("v1", report)
        self.assertIn("v2", report)
        
        # ë³´ê³ ì„œ ì €ì¥ í…ŒìŠ¤íŠ¸
        report_file = self.performance_comparator.save_comparison_report(report)
        self.assertTrue(os.path.exists(report_file))
        
        print("âœ… ë¹„êµ ë³´ê³ ì„œ ìƒì„± ì„±ê³µ")
    
    def test_optimization_report_generation(self):
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“‹ ìµœì í™” ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ìš© ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        test_recommendations = [
            OptimizationRecommendation(
                id="test_001",
                category=OptimizationCategory.CPU,
                priority=OptimizationPriority.CRITICAL,
                title="ê¸´ê¸‰ CPU ìµœì í™”",
                description="CPU ì‚¬ìš©ë¥  ê¸´ê¸‰ ìµœì í™”",
                impact_description="ì‹œìŠ¤í…œ ì•ˆì •ì„± í–¥ìƒ",
                implementation_steps=["1ë‹¨ê³„", "2ë‹¨ê³„"],
                estimated_improvement="30% ê°œì„ ",
                risk_level="ë‚®ìŒ",
                created_at=datetime.now()
            ),
            OptimizationRecommendation(
                id="test_002",
                category=OptimizationCategory.MEMORY,
                priority=OptimizationPriority.HIGH,
                title="ë©”ëª¨ë¦¬ ì •ë¦¬",
                description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”",
                impact_description="ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ",
                implementation_steps=["ì •ë¦¬", "ìµœì í™”"],
                estimated_improvement="20% ê°œì„ ",
                risk_level="ë‚®ìŒ",
                created_at=datetime.now(),
                applied=True,
                applied_at=datetime.now()
            )
        ]
        
        self.performance_optimizer.recommendations.extend(test_recommendations)
        
        # ìµœì í™” ë³´ê³ ì„œ ìƒì„±
        report = self.performance_optimizer.generate_optimization_report()
        
        # ë³´ê³ ì„œ ë‚´ìš© ê²€ì¦
        self.assertIsInstance(report, str)
        self.assertIn("ìµœì í™” ë³´ê³ ì„œ", report)
        self.assertIn("ìµœì í™” ìš”ì•½", report)
        self.assertIn("ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„", report)
        self.assertIn("ìš°ì„ ìˆœìœ„ë³„ ë¶„ì„", report)
        self.assertIn("ìµœê·¼ ê¶Œì¥ì‚¬í•­", report)
        self.assertIn("ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­", report)
        
        print("âœ… ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì„±ê³µ")


def run_performance_integration_test():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ POSCO ì›Œì¹˜í–„ìŠ¤í„° v2 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±
    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestPerformanceMonitoring)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"ì´ í…ŒìŠ¤íŠ¸: {result.testsRun}")
    print(f"ì„±ê³µ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"ì‹¤íŒ¨: {len(result.failures)}")
    print(f"ì˜¤ë¥˜: {len(result.errors)}")
    
    if result.failures:
        print("\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
        for test, traceback in result.failures:
            error_msg = traceback.split('AssertionError: ')[-1].split('\n')[0]
            print(f"  â€¢ {test}: {error_msg}")
    
    if result.errors:
        print("\nğŸ’¥ ì˜¤ë¥˜ê°€ ë°œìƒí•œ í…ŒìŠ¤íŠ¸:")
        for test, traceback in result.errors:
            error_msg = traceback.split('\n')[-2]
            print(f"  â€¢ {test}: {error_msg}")
    
    # ì „ì²´ ê²°ê³¼
    if result.wasSuccessful():
        print("\nğŸ‰ ëª¨ë“  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        return True
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False


if __name__ == '__main__':
    success = run_performance_integration_test()
    sys.exit(0 if success else 1)